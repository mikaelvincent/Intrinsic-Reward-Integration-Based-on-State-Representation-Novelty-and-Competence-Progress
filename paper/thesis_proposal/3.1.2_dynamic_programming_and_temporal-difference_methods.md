#### 3.1.2 Dynamic Programming and Temporal-Difference Methods

In tabular or low-dimensional RL, the agent can compute \(V^*\) or \(Q^*\) via dynamic programming (Szepesvári, 2010) or temporal-difference (TD) methods (Tsitsiklis & Van Roy, 1997). Value iteration iterates:
\[
Q_{k+1}(s,a) \;=\; r(s,a)\;+\;\gamma \,\mathbb{E}\Bigl[\max_{a'}Q_k(s',a')\Bigr],
\]
converging to \(Q^*(s,a)\). For approximate or function-approximation settings (Peters & Schaal, 2008; Tsitsiklis & Van Roy, 1997), we store \(Q(s,a;\theta)\) in parametric form. TD learning updates \(\theta\) incrementally, e.g., for Q-learning:
\[
Q(s_t,a_t;\theta) \;\leftarrow\; Q(s_t,a_t;\theta)\;+\;\alpha\Bigl[r_{t+1} \;+\;\gamma\,\max_{a'}Q(s_{t+1},a';\theta)\;-\;Q(s_t,a_t;\theta)\Bigr].
\]

While basic Q-learning or TD methods are guaranteed to converge in tabular contexts, they can be unstable with function approximation. Results by Tsitsiklis & Van Roy (1997) and Szepesvári (2010) show that additional constraints (e.g., linear approximators under on-policy sampling) are required to ensure convergence.
