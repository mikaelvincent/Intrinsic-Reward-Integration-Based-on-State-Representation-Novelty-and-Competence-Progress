#### 4.01.5 Respondents, Learning Agents, and Training Protocol

We designate the learning agents (i.e., RL policies) as the “respondents” in these experiments, consistent with naming in some methodological frameworks (Oudeyer & Kaplan, 2007). Each agent:
- Architecture & Training
  - Uses a standard policy gradient or actor-critic algorithm, e.g., Proximal Policy Optimization (PPO), Soft Actor-Critic (SAC), or an advanced natural policy gradient (Kakade, 2001).
  - Integrates one or more intrinsic reward signals proposed in the next sections (Baranes & Oudeyer, 2009; Ostrovski et al., 2017; Raileanu & Rocktäschel, 2020; Pathak et al., 2019).
  - Trajectories are collected in parallel across multiple environment seeds, following best practices for sample efficiency (Burda, Edwards, Pathak, et al., 2018).
- Intrinsically Motivated Behaviors
  - We incorporate an ensemble-based or Bayesian approach (Houthooft et al., 2016; Pathak et al., 2019) to discount purely random transitions, plus a competence-based module (Baranes & Oudeyer, 2009; Eysenbach et al., 2018) to measure skill mastery.
  - Agent hyperparameters—like learning rate and entropy regularization—are tuned to ensure stable off-policy or on-policy learning across random seeds (Mazzaglia et al., 2021).
- Lifecycle
  - Each agent training run constitutes an entire “respondent” experience from initialization to final convergence.
  - The partial randomness in environments ensures different episodes rarely share the same arrangement (Sekar et al., 2020; Jarrett et al., 2022).
