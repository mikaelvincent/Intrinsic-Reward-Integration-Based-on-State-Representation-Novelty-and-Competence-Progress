### 1.2 Statement of the Problem

Reinforcement learning agents frequently fail to maintain effective exploration in environments where certain transitions are irreducibly random (Pathak et al., 2017). Simple novelty-seeking methods (Bellemare et al., 2016) and count-based approaches (Tang et al., 2016) often overemphasize these uncontrollable transitions, leading to infinite loops or rapidly diminished performance. Learning progress estimators such as R-IAC (Baranes and Oudeyer, 2009) address some of these shortcomings by prioritizing regions showing improvement in forward-model accuracy, but existing methods do not systematically identify partially random transitions at scale.

Sparse or delayed extrinsic rewards amplify these exploration challenges, especially when large portions of the environment exhibit partial randomness. Agents either linger in unlearnable subregions or waste significant effort on transitions that offer no meaningful information gain (Mavor-Parker et al., 2021). Despite recent progress unifying intrinsic reward signals (Oudeyer and Kaplan, 2007), no single framework robustly addresses partial randomness across a variety of RL domains, both discrete and continuous.

Accordingly, a need persists for an adaptive curiosity mechanism that integrates multiple intrinsic signals, discounts genuinely unlearnable transitions, and promotes sustained exploration. Such a mechanism must reconcile novelty-based incentives with learning progress and distributional awareness, ensuring that exploration remains directed toward controllable, knowledge-yielding aspects of the environment.
