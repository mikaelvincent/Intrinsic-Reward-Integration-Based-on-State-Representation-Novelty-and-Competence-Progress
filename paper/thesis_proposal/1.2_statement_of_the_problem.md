### 1.2 Statement of the Problem

Reinforcement learning (RL) has shown tremendous potential for enabling agents to learn complex behaviors and master challenging domains. However, in many real-world and simulated tasks where the environment contains partial randomness—be it in dynamics, observations, or procedural generation—naïve exploration schemes (e.g., \(\epsilon\)-greedy, Gaussian noise) fail to systematically uncover effective policies (Bellemare et al., 2016; Mavor-Parker et al., 2021; Pathak et al., 2019). Traditional novelty-driven exploration approaches often collapse in the presence of irreducible noise (the so-called “noisy-TV” effect), leading the agent to overfocus on uncontrollable transitions that yield perpetual high curiosity signals (Houthooft et al., 2016). 

Such failures underscore the need for an adaptive curiosity mechanism that can:
- Distinguish between learnable vs. purely random transitions.
- Continually explore new subregions where it can improve its internal models (Baranes & Oudeyer, 2009; Oudeyer & Kaplan, 2007).
- Reinforce complex or hierarchical behaviors beyond momentary novelty (Eysenbach et al., 2018; Gregor et al., 2016).

Despite multiple existing curiosity or intrinsic motivation methods—ranging from state novelty (Bellemare et al., 2016) and model-disagreement (Pathak et al., 2019) to competence progress (Baranes & Oudeyer, 2009)—a systematic and unified framework that robustly handles partial randomness while driving sustained skill discovery remains elusive. Many methods fixate on ephemeral noise (Mavor-Parker et al., 2021), saturate their exploration bonus prematurely (Burda, Edwards, Pathak, et al., 2018), or struggle in large, procedurally generated domains (Raileanu & Rocktäschel, 2020).

Hence, this study addresses the following overarching problem:
> How can an RL agent, operating in environments with partial randomness and sparse external rewards, systematically prioritize learnable transitions, discount irreducible noise, and build an evolving repertoire of robust behaviors guided by a multi-signal intrinsic reward scheme?

We pose that an advanced adaptive curiosity framework—integrating knowledge-based Bayesian signals, noise-aware forward-model corrections, and competence-based progress—can effectively drive exploration and skill acquisition in partially random or procedurally generated RL environments.
