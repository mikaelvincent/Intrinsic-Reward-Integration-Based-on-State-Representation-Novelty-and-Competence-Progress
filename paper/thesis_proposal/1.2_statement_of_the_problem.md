### 1.2 Statement of the Problem

Reinforcement learning agents frequently fail to maintain effective exploration in environments where certain transitions are irreducibly random. Simple novelty-seeking methods (Pathak et al., 2017) and count-based approaches (Bellemare et al., 2016) often overemphasize these uncontrollable transitions, leading to infinite loops or rapidly diminished performance. Learning progress estimators such as R-IAC (Baranes and Oudeyer, 2009) mitigate some of these shortcomings by focusing on regions where predictions show improvement, but existing methods do not systematically filter out random transitions at scale.

Sparse or delayed extrinsic rewards further exacerbate these exploration challenges. When large portions of the environment are partially random, agents either linger in random subregions or waste significant effort on transitions that offer no meaningful knowledge gain. Despite recent progress in unifying intrinsic reward signals (Oudeyer and Kaplan, 2007), no single framework robustly addresses partial randomness across diverse continuous and discrete domains.

Hence, a need persists for an adaptive curiosity mechanism that integrates multiple intrinsic signals, discounts genuinely unlearnable transitions, and promotes sustained exploration. Such a mechanism must systematically reconcile impact-based novelty with learning progress and distributional awareness, ensuring that exploration remains directed toward controllable, knowledge-yielding aspects of the environment.
