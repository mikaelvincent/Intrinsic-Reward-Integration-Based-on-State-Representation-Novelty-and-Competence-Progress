### 1.2 Statement of the Problem

Reinforcement learning (RL) has demonstrated significant potential in enabling agents to learn complex behaviors and solve challenging tasks. However, many real-world and simulated environments are characterized by partial randomness—whether in their dynamics, observations, or procedural generation. In such scenarios, traditional exploration strategies (e.g., \(\epsilon\)-greedy or Gaussian noise injection) are insufficient for systematically uncovering effective policies (Bellemare et al., 2016; Mavor-Parker et al., 2021; Pathak et al., 2019). Similarly, conventional intrinsic motivation methods, which rely on novelty-driven exploration, frequently fail in these environments, becoming trapped by irreducible stochasticity and continuously high surprise signals—a phenomenon known as the "noisy-TV" effect (Houthooft et al., 2016).

These limitations highlight the critical need for an adaptive curiosity mechanism that can effectively distinguish between learnable transitions and purely random, uncontrollable aspects of an environment. Ideally, such a mechanism would guide an agent toward exploring novel, yet meaningful, states and subregions that can enhance its internal models and foster complex, hierarchical behaviors beyond momentary novelty (Baranes & Oudeyer, 2009; Oudeyer & Kaplan, 2007; Eysenbach et al., 2018; Gregor et al., 2016).

Despite advances in intrinsic motivation methods—including approaches based on state novelty, ensemble disagreement, and competence progress—a robust and unified framework that adequately handles partial randomness remains elusive. Existing methods often suffer from critical shortcomings, such as fixation on irreducible noise (Mavor-Parker et al., 2021), premature saturation of intrinsic rewards (Burda, Edwards, Pathak, et al., 2018), or inadequate performance in procedurally generated and large-scale domains (Raileanu & Rocktäschel, 2020).
