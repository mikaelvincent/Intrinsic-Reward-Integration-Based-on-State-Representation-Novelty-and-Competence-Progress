#### 2.5.3 Self-Supervised World Models and Planning

Another line of work uses self-supervised model learning to facilitate more directed exploration and adaptation. For example, Plan2Explore (Sekar et al., 2020) pretrains a world model by interacting with the environment in the absence of extrinsic rewards. An ensemble-based disagreement measure in latent space signals epistemic uncertainty, guiding the agent to gather data in underexplored regions. Once a new task’s reward function becomes available, the agent adapts rapidly by “planning to explore” in its learned model. This synergy of self-supervised world models and curiosity-driven coverage has proven fruitful in partially random or procedurally generated tasks where repeated rollouts are expensive or state distributions shift unpredictably.
