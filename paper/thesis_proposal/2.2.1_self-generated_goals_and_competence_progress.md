#### 2.2.1 Self-Generated Goals and Competence Progress

A hallmark of competence-based approaches is the agent’s ability to propose goals or tasks on its own, then evaluate how successfully (or how much better) it achieves them compared to the past. Under this scheme, an intrinsic reward is tied to the **learning progress** or **improvement** in accomplishing selected goals (Oudeyer & Kaplan, 2007). Rather than constantly seeking the most “novel” or “unpredictable” states, the agent targets regions or tasks at the “right level of challenge,” aligning with concepts of **optimal challenge** or **flow** in developmental robotics.

- **R-IAC (Robust Intelligent Adaptive Curiosity)**  
  An influential example is **R-IAC** (Baranes & Oudeyer, 2009). It partitions the space of possible goals (sensorimotor configurations) into subregions and tracks improvement in each. If the agent sees large learning progress in a subregion, it continues exploring there; if the agent’s error stagnates or the region proves unlearnable, R-IAC shifts focus. By incrementally discovering which parts of the environment yield sustained competence gains, the agent avoids random or unproductive behaviors.

- **Goal-Based Intrinsic Motivation**  
  Competence-based methods often treat “goals” as explicit configurations (e.g., positions, poses) or sub-tasks. The agent’s intrinsic reward typically depends on how close it gets to the target, or how much the closeness improves. This approach can sidestep the pitfalls of purely local surprise in partially random domains, since the agent focuses on tasks it can incrementally master.
