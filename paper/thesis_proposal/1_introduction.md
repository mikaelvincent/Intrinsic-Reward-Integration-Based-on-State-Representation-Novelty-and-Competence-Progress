## 1. Introduction

In this chapter, we provide an overview of the rationale, objectives, significance, and scope of our research on adaptive curiosity mechanisms for exploration in partially random reinforcement learning (RL) environments. We begin by discussing the need for effective intrinsic motivation approaches capable of overcoming exploration challenges posed by partial randomness and sparse external rewards. Next, we explicitly state the research problem and our corresponding general and specific objectives, highlighting our goal to develop a unified adaptive curiosity framework that integrates knowledge-based Bayesian signals, noise-aware corrections, and competence-driven progress measures. We then emphasize the potential significance of our approach in advancing robust exploration and skill acquisition strategies within procedurally generated or stochastic RL domains. Lastly, we outline the scope and limitations of our study, clarifying the specific environments, methodologies, and constraints under consideration.
