## 1. Introduction

Adaptive curiosity—an intrinsic drive to seek out learnable challenges or reduce one’s uncertainty—has become a cornerstone of modern exploration research in reinforcement learning (RL), especially in tasks where extrinsic rewards are sparse, deceptive, or partially random (Achiam et al., 2018; Baranes & Oudeyer, 2009; Bellemare et al., 2016; Eysenbach et al., 2018; Groth et al., 2021; Houthooft et al., 2016; Huang et al., 2021; Mavor-Parker et al., 2021; Oudeyer & Kaplan, 2007; Pathak et al., 2019; Raileanu & Rocktäschel, 2020). In partially random environments, naïve novelty-seeking agents risk indefinite fixation on uncontrollable or noise-dominated transitions, undermining their ability to discover meaningful subgoals (Mavor-Parker et al., 2021; Pathak et al., 2019). Knowledge-based approaches, such as Bayesian information gain (Houthooft et al., 2016), ensemble disagreement (Pathak et al., 2019), or subtracting predicted noise (Mavor-Parker et al., 2021), have shown promise in recognizing and discounting irreducible stochasticity, while competence-based methods (Baranes & Oudeyer, 2009; Eysenbach et al., 2018) help agents master tasks or subgoals they can truly influence (Groth et al., 2021). Moreover, hybrid or multi-signal strategies (Huang et al., 2021; Raileanu & Rocktäschel, 2020) combine state novelty, forward-model errors, or skill-discovery bonuses to achieve robust exploration, underscoring the advantages of integrating multiple curiosity signals. Building upon these insights, the present study aims to unify key elements from knowledge-based, noise-aware, and competence-based frameworks into a single adaptive curiosity mechanism. By systematically handling partial randomness while encouraging genuine skill progression, we hypothesize that the agent will avoid noisy dead-ends, explore relevant subregions more efficiently, and develop reusable behaviors for complex, open-ended RL tasks.
