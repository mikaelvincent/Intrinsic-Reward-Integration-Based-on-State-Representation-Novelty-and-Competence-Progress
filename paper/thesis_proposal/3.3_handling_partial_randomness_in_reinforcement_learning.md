### 3.3 Handling Partial Randomness in Reinforcement Learning

In many real-world tasks, environment transitions include both *learnable* (or structured) components and *partially random* elements, posing a fundamental challenge for exploration. Naive exploration methods—such as \(\varepsilon\)-greedy actions, raw prediction-error curiosity, or uniform policy sampling—often fail when large regions of the state space yield irreducibly stochastic outcomes. Agents might repeatedly visit uncontrollable transitions, receiving misleading novelty signals (“noisy TV” effect) and neglecting states where genuine learning progress is achievable (Mavor-Parker et al., 2021). Consequently, an agent that treats *all* surprising transitions as equally useful can waste significant time in purely random loops.
