#### 2.1.2 Model Prediction Errors and Forward Dynamics

Instead of tracking how frequently states occur, a predictive exploration approach looks at how well an agent’s learned forward model can predict the next state given the current state and action. Discrepancies between predicted and actual observations form an intrinsic signal—often called a curiosity or surprise bonus (Pathak et al., 2017; Groth et al., 2021). The agent is thus drawn to transitions for which its predictive model is uncertain or systematically in error. Two influential examples are:
- Inverse and Forward Dynamics (ICM)
  - An inverse model discerns which action led from one state to another, while a forward model predicts the next state’s latent embedding. The agent is intrinsically rewarded for large prediction errors (Pathak et al., 2017). Over time, once certain transitions become predictable, the intrinsic reward there diminishes, steering the agent toward novel or unexplored outcomes.
- Random Network Distillation (RND)
  - A target network is fixed at initialization, while a predictor network trains to match the target’s outputs for new states (Burda, Edwards, Pathak, et al., 2018; Burda, Edwards, Storkey, & Klimov, 2018). High predictor error signals unfamiliar states, functioning like a running novelty metric.

Both lines of research exploit the idea that unlearned transitions yield meaningful intrinsic reward, guiding exploration in sparse or deceptive tasks. A key challenge, however, is avoiding indefinite attraction to purely random or unlearnable aspects of the environment (the “noisy TV” effect). Methods such as ensemble disagreement (Pathak et al., 2019) or subtracting aleatoric uncertainties (Mavor-Parker et al., 2021) have been proposed to address uncontrollable randomness.
