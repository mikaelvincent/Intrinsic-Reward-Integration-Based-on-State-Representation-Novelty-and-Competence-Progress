### 3.1 Reinforcement Learning Formulation

Reinforcement learning problems often use a Markov decision process (MDP) defined by $\langle \mathcal{S}, \mathcal{A}, p, r, \gamma \rangle$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $p$ is the transition distribution, $r$ is the extrinsic reward function, and $\gamma \in [0,1)$ is a discount factor (Szepesvári, 2010). An agent's policy $\pi(a \mid s)$ maps states to action probabilities. The goal is typically to maximize the expected discounted return $\mathbb{E}\bigl[\sum_{t=0}^\infty \gamma^t\,r(s_t,a_t)\bigr]$.

In partially random environments, certain state transitions exhibit irreducible stochasticity or "noisy TV" artifacts (Burda, Edwards, Pathak, et al., 2018). Simple $\epsilon$-greedy or parameter-noise exploration (Tang et al., 2016) may stall or waste many steps. Intrinsic rewards offer an alternative, focusing the agent on transitions that improve a learned model (Houthooft et al., 2016) or highlight agent-induced changes (Raileanu & Rocktäschel, 2020).
