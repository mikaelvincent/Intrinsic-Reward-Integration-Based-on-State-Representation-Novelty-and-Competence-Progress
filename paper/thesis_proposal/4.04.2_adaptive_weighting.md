#### 4.04.2 Adaptive Weighting

We unify these signals in a single composite intrinsic reward:
\[
  r^i_t
  \;=\;
  w_{\mathrm{EBIG}}(t)\;r^i_{\mathrm{EBIG}}(t)
  \;+\;
  w_{\mathrm{ANAPE}}(t)\;r^i_{\mathrm{ANAPE}}(t)
  \;+\;
  w_{\mathrm{CBSP}}(t)\;r^i_{\mathrm{CBSP}}(t),
\]
where \(w_{\mathrm{EBIG}}, w_{\mathrm{ANAPE}}, w_{\mathrm{CBSP}}\) are dynamic weights updated online. Inspired by weighting ideas in multi-signal curiosity (Huang et al., 2021), we adopt three key strategies for adjusting these reward components dynamically:
- Policy-Aware
  - If the agent’s policy is highly stochastic (high entropy), we momentarily emphasize knowledge-based signals (EBIG, ANAPE) to encourage coverage.  
- Distribution-Aware
  - If certain transitions are rarely visited, we may amplify competence-based signals in those subregions, accelerating skill discovery.  
- Smoothing
  - We track a moving average of each component’s magnitude to keep each weight stable and avoid abrupt oscillations in the final reward.
