#### 4.5.3 Hyperparameter Configuration

Hyperparameter values build upon recommended defaults from the prior literature on curiosity-driven or count-based exploration (Bellemare et al., 2016; Pathak et al., 2017; Raileanu and Rocktäschel, 2020).

The key hyperparameters include:
1. RL Algorithm
   - PPO: learning rate $\alpha \approx 3\times10^{-4}$, $\epsilon=0.2$ clipping range, 2048 steps per update, mini-batch size 64–256, advantage estimator $\lambda=0.95$, discount $\gamma=0.99$.
2. Intrinsic Reward Scaling ($\eta$)
   - Balances external reward $r^{\mathrm{ext}}$ with curiosity-based $r^{\mathrm{int}}$. Typical values range 0.01–0.2, consistent with (Houthooft et al., 2016; Raileanu and Rocktäschel, 2020). Tuning is performed environment-by-environment.
3. Forward / Inverse Model Networks (in methods similar to ICM or RIDE)
   - Two to four-layer MLP with 256 hidden units or CNN stacks for image-based tasks.
   - Learning rates: $3\times10^{-4}$ or $1\times10^{-3}$.
   - Weighting of forward vs. inverse losses: typically 1:1, though environment-specific tweaks (0.8–1.2 ratio) can arise from grid searches (Pathak et al., 2017).
4. Count / Density-Based Approaches (if used in baselines)
   - Pixel-based hashing or downsampled frames (42×42) for pseudo-count extraction (Bellemare et al., 2016).
   - Normalization and channel-wise scaling are employed to keep novelty estimates stable (Burda et al., 2018).
5. Ensemble Disagreement (if used in ablations)
   - 3–5 neural networks predicting $\hat{s}_{t+1}$ from $(s_t, a_t)$. The ensemble variance in $\hat{s}_{t+1}$ yields the intrinsic reward (Pathak et al., 2019).
   - Each network uses 2–3 hidden layers, 256 units, ReLU activations.
6. Environment-Specific Adjustments
   - As documented, certain tasks require environment-specific scaling for intrinsic bonus, e.g., Car Racing ($\eta=0.05$) or Bipedal Walker ($\eta=0.1$) (Burda et al., 2018).
   - Episode horizon limits: Mountain Car (200 steps), Bipedal Walker (1600 steps), and so forth, are consistent with standard open-source definitions.

These hyperparameters are chosen through limited random or grid-based searches, guided by typical ranges from prior references (Bellemare et al., 2016; Pathak et al., 2017). Each environment uses either a single stable configuration or a small set of validated parameter sets. Sane defaults (e.g., $\gamma=0.99$, $\alpha=3\times10^{-4}$) rarely require major adjustments.
