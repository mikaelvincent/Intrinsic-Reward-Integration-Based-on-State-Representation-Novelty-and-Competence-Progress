### 3.1 Foundations of Modern Reinforcement Learning

Modern reinforcement learning (RL) addresses the problem of how an agent can learn to map **states** \( s \in \mathcal{S} \) to **actions** \( a \in \mathcal{A} \) so as to maximize a numerical return or reward (Sutton & Barto, 2018; Szepesvári, 2010). Formally, we typically model the environment as a Markov Decision Process (**MDP**), defined by a tuple:

\[
\mathcal{M} \;=\; \langle \mathcal{S}, \;\mathcal{A}, \;P, \;r, \;\gamma \rangle,
\]

where:

- \(\mathcal{S}\) is the (possibly high-dimensional) **state space**;
- \(\mathcal{A}\) is the **action space** (discrete or continuous);
- \(P(\cdot \mid s,a)\) is the **transition distribution** over next states, describing the environment’s dynamics;
- \(r(s,a)\) or \(r(s,a,s')\) is the **reward function**, which can be deterministic or stochastic;
- \(0 \le \gamma < 1\) is the **discount factor** controlling how heavily future returns are weighed.

The *goal* of the agent is to find a **policy** \(\pi\), mapping states to actions (either stochastically, \(\pi(a \mid s)\), or deterministically, \(\pi(s)\)), which **maximizes** the **expected return**:

\[
J(\pi) \;=\; \mathbb{E}_{\tau \sim \pi}\Bigl[\;\sum_{t=0}^{\infty}\,\gamma^t\,r(s_t,a_t)\Bigr],
\]

where \(\tau = (s_0,a_0,s_1,a_1,\dots)\) is a trajectory generated by following \(\pi\) in the MDP.
