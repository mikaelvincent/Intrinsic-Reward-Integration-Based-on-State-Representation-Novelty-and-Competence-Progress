### 4.3 Approaches for Weighting Multiple Intrinsic Rewards

In our setup, three intrinsic reward signals are combined alongside external task rewards: (1) an ensemble-based disagreement bonus for epistemic uncertainty, (2) an aleatoric noise subtraction term to discount uncontrollable environment randomness, and (3) a competence-based progress bonus capturing the agent's improvement on self-generated subgoals (see Pathak et al., 2017; Burda et al., 2018; Oudeyer and Kaplan, 2007). Integrating these signals in a single exploration objective requires a mechanism to adaptively balance or unify them, preventing any one signal from dominating or vanishing prematurely.

We consider two main strategies for weighting these signals at each environment step \(t\). Let \(r_{\mathrm{disagree}}(t)\) denote the ensemble disagreement term, \(r_{\mathrm{aleatoric}}(t)\) the aleatoric-noise-subtracted forward error, and \(r_{\mathrm{comp}}(t)\) the competence-based progress measure. We also define a short-term measure of policy entropy \(H(\pi_t)\), and a state-distribution factor that depends on visit counts \(n_s\bigl(s_t\bigr)\) or cluster-based rarity. Each strategy multiplies the raw intrinsic terms by a time-varying coefficient or coefficients derived from these policy and distribution features.

1. Approach A (Single Unified Formula). We define a single dynamic coefficient \(\alpha_j(t)\) that depends on both the policy's action-entropy and the distribution of recently visited states. One might set
   \[
     \alpha_j(t) \;=\; f\Bigl(H(\pi_t)\Bigr)\;\times\;g\Bigl(n_s\bigl(s_t\bigr)\Bigr),
   \]
   where \(f\bigl(\cdot\bigr)\) and \(g\bigl(\cdot\bigr)\) are monotonic functions designed so that \(\alpha_j(t)\) increases when the agent's policy is uncertain (high entropy) or the state is rarely visited. A common choice is to let \(f\) linearly scale the entropy to a \([0,1]\) range and let \(g\) exponentially decay with increasing visitation counts. The overall intrinsic reward then becomes
   \[
     r_{\mathrm{int}}(t)
     \;=\;
     \alpha_j(t)\;
     \Bigl[\,
       r_{\mathrm{disagree}}(t)
       \;+\;
       r_{\mathrm{aleatoric}}(t)
       \;+\;
       r_{\mathrm{comp}}(t)
     \Bigr].
   \]
   In practice, each sub-signal can be normalized by its running mean-std to ensure commensurate scales before applying \(\alpha_j(t)\). This single-factor design keeps the final bonus simpler in form, at the expense of less fine-grained control over individual signals.

2. **Approach B (Separate Coefficients).** In a more flexible alternative, two distinct dynamic factors are introduced. One factor \(\beta_t\) (often tied to policy entropy) applies to knowledge-based signals (the ensemble disagreement and aleatoric terms), while another factor \(\eta_t\) (often tied to underexplored states) applies to the competence-based progress:
   \[
     r_{\mathrm{int}}(t)
     \;=\;
     \beta_t\,
     \Bigl[
       r_{\mathrm{disagree}}(t)
       \;+\;
       r_{\mathrm{aleatoric}}(t)
     \Bigr]
     \;+\;
     \eta_t\,
     r_{\mathrm{comp}}(t).
   \]
   Here, \(\beta_t\) might be set as some function of the current action-entropy \(\mathrm{Entropy}\bigl(\pi_t\bigr)\), encouraging greater emphasis on uncertainty-driven curiosity when the policy is not yet confident. Meanwhile, \(\eta_t\) can decay as the local visitation count \(n_s\bigl(s_t\bigr)\) grows, focusing competence-based exploration on subregions not yet mastered. Such a decoupled strategy can keep knowledge-based and competence-based signals from interfering, but requires two separate heuristics for \(\beta_t\) and \(\eta_t\).

In both approaches, we compute these weighting factors "on the fly," using the same rollouts collected by our on-policy agent (cf. Pathak et al., 2017; Burda et al., 2018). Each intrinsic sub-signal is also normalized by a running mean and standard deviation. We add the resulting weighted bonus to the extrinsic reward at each step, storing it in the Proximal Policy Optimization buffer for standard advantage estimation and policy-gradient updates. Further, we measure and log raw ensemble-disagreement magnitudes, predicted covariance from the forward models, and incremental changes in subgoal success rates for competence progress. This ensures we can track how each intrinsic component contributes to exploration and whether any single term vanishes or dominates during training.

These weighting formulas can be adjusted if initial experiments show imbalances among the sub-signals (for instance, if ensemble disagreement saturates in high-dimensional spaces or if competence-based progress overwhelms other bonuses in simpler puzzles). Our ablation studies on the Key Corridor and Ant Maze tasks specifically test removing or fixing these weights to evaluate each signal's impact on exploration success.

