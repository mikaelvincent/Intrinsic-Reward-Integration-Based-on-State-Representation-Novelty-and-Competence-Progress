#### 3.2.5 Mathematical Synthesis

Below is a schematic synthesis of key *intrinsic reward* definitions, illustrating how each formula addresses partial randomness:

1. **State-Novelty (Count/Hash)**:
   \[
     r^i_t \;=\;\bigl(N(s_{t+1}))^{-\frac{1}{\alpha}}
     \quad \text{or}\quad
     r^i_t \;=\;\bigl(\hat{p}(s_{t+1})\bigr)^{-\gamma},
   \]
   where \(\hat{p}\) is a learned density or hashing-based measure (Tang et al., 2016).

2. **Forward Error**:
   \[
     r^i_t \;=\;\bigl\|f_\theta(s_t,a_t)-s_{t+1}\bigr\|^\kappa, 
   \]
   or its inverse form \(\tfrac{1}{\|f_\theta(\cdot)-\cdot\|}\). High errors occur in novel transitions.

3. **Info Gain (VIME)**:
   \[
     r^i_t \;=\;
       \mathbb{E}_{s_{t+1}}
       \Bigl[D_{\mathrm{KL}}\bigl(q_{t+1}(\theta)\,\big\|\,q_t(\theta)\bigr)\Bigr],
   \]
   capturing how the BNN posterior updates after each transition (Houthooft et al., 2016).

4. **Ensemble Disagreement**:
   \[
     r^i_t \;=\;\mathrm{Var}_k\bigl[f_k(s_t,a_t)\bigr],
   \]
   effectively ignoring purely random transitions once the ensemble predictions converge to the same distribution (Pathak et al., 2019).

5. **Learning Progress**:
   \[
     r^i_t \;=\;\Delta\Bigl(\text{error region}\Bigr),
     \quad
     \text{with region-based old vs. new errors.}
   \]
   Once a subregion is either “mastered” or discovered unlearnable, progress saturates, pushing exploration elsewhere (Baranes & Oudeyer, 2009).

6. **Skill Discovery**:
   \[
     \max_{(\pi_\theta,q)}
       \mathbb{E}_{z,\,\tau\sim\pi_\theta(\cdot\mid z)}
       \Bigl[\log q(z\mid\tau)\Bigr] + H\bigl(\pi_\theta(\cdot\mid z)\bigr),
   \]
   forcing distinct latent skills to produce easily distinguishable trajectories (Achiam et al., 2018; Gregor et al., 2016; Eysenbach et al., 2018).
