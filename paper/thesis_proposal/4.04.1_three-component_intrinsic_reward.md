#### 4.04.1 Three-Component Intrinsic Reward

The proposed framework introduces three intrinsic signals. Let \(\mathbf{s}_t\) and \(\mathbf{a}_t\) denote the agent’s state and action at time \(t\), and let \(\mathbf{s}_{t+1}\) be the next state. Each transition \(\bigl(\mathbf{s}_t,\mathbf{a}_t,\mathbf{s}_{t+1}\bigr)\) yields an intrinsic reward \(r^i_t\) which supplements any extrinsic reward \(r^e_t\). Formally, the agent’s total reward is:
\[
  r'_t
  \;=\;
  r^e_t
  \;+\;
  r^i_t
  \;=\;
  r^e_t
  \;+\;
  \alpha \, r^i_t,
\]
where \(\alpha\) is a scaling factor. Our intrinsic component \(r^i_t\) is composed of:
- Bayesian Ensemble Information Gain (\(\mathrm{IG}\)-Ensemble or \(\mathrm{EBIG}\))
  - Extends the idea of variational information maximizing exploration (Houthooft et al., 2016) and ensemble disagreement methods (Pathak et al., 2019).
  - Maintains an ensemble of forward dynamics models \(\{f_k\}_{k=1}^K\). Each model \(f_k\) is trained to predict \(\mathbf{s}_{t+1}\) from \(\mathbf{s}_t,\mathbf{a}_t\).
  - After each new transition, we update the ensemble’s posterior or (approximate) Bayesian parameters \(\{\theta_k\}\). The agent’s information gain about these parameters from observing \((\mathbf{s}_t,\mathbf{a}_t,\mathbf{s}_{t+1})\) forms a curiosity bonus:
     \[
       r^i_{\mathrm{EBIG}}(t)
       \;=\;
       \mathbb{D}_\mathrm{KL}\Bigl[
         p(\theta\mid \mathcal{D}_{t+1})
         \;\big\|\;
         p(\theta\mid \mathcal{D}_{t})
       \Bigr],
     \]
     approximated in practice via ensemble disagreement or a small KL-based step. Notably, purely random transitions eventually yield low information gain, as all models converge to the same uncertain prediction for uncontrollable noise (Pathak et al., 2019).
- Aleatoric-Subtracted Prediction Error (\(\mathrm{ANAPE}\))
  - Adapts the approach of aleatoric mapping agents (Mavor-Parker et al., 2021). Each forward model predicts both the mean \(\hat{\mu}_{t+1}\) and variance \(\hat{\Sigma}_{t+1}\).
  - The raw curiosity from forward-model error, \(\|\mathbf{s}_{t+1}-\hat{\mu}_{t+1}\|^2,\) is discounted by \(\mathrm{Tr}(\hat{\Sigma}_{t+1})\), subtracting predicted noise:
     \[
       r^i_{\mathrm{ANAPE}}(t)
       \;=\;
       \|\mathbf{s}_{t+1}-\hat{\mu}_{t+1}\|^2
       \;-\;
       \eta\,\mathrm{Tr}\Bigl(\hat{\Sigma}_{t+1}\Bigr),
     \]
     where \(\eta>0\). Transitions recognized as purely random (high \(\hat{\Sigma}\)) yield minimal net curiosity, preventing indefinite loops on noisy states (the “noisy TV” problem).
- Competence-Based Skill Progress (\(\mathrm{CBSP}\))
  - Builds on R-IAC (Baranes & Oudeyer, 2009), DIAYN (Eysenbach et al., 2018), and VIC (Gregor et al., 2016). The agent periodically partitions its state or goal space into subregions, each associated with a local measure of skill competence.
  - The agent’s intrinsic reward reflects gains in skill mastery or reliability. Formally, if \(\mathrm{error}_{\mathcal{R}}\) is the skill error in region \(\mathcal{R}\) before and after an update, then:
     \[
       r^i_{\mathrm{CBSP}}(t)
       \;=\;
       \mathrm{error}^{\mathrm{old}}_{\mathcal{R}}
       \;-\;
       \mathrm{error}^{\mathrm{new}}_{\mathcal{R}},
     \]
     awarding exploration where the agent is actively improving its competence.
