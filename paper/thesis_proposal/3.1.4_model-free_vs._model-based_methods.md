#### 3.1.4 Model-Free vs. Model-Based Methods

A fundamental distinction is whether the agent **learns a model** \(P_\phi(s_{t+1}\mid s_t,a_t)\) of environment dynamics:

- **Model-Free RL**: Directly learns value functions or policies from experience (e.g., Q-learning, policy gradients). Typically simpler to implement but may require more samples.
- **Model-Based RL**: Trains an approximate environment model \(P_\phi\), then **plans** or **searches** in that learned model (Houthooft et al., 2016; Sekar et al., 2020). Potentially more sample-efficient, but modeling errors can degrade performance.

Some advanced frameworks, such as **VIME** (Variational Information Maximizing Exploration) (Houthooft et al., 2016), maintain a *Bayesian* neural network for model-based uncertainty and derive an *intrinsic reward* from *information gain* about the dynamics. Similarly, **Plan2Explore** (Sekar et al., 2020) plans in a latent space to explore uncertain regions, then rapidly adapts to tasks with minimal additional data.
