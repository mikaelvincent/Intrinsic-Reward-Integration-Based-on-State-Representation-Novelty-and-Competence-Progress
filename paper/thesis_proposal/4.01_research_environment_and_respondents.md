### 4.01 Research Environment and Respondents

To systematically investigate the effectiveness of our curiosity-driven framework, we select environments characterized by partial randomness in transitions, object placements, and layouts. These environments enable precise measurement of exploration behavior, competence development, and robustness to noise. Our chosen tasks include procedurally generated grid-based mazes, two-dimensional maze navigation tasks with stochastic dynamics, procedural dungeon environments, and high-dimensional continuous control scenarios with random forces.

For grid-based maze environments, we use the MiniGrid suite (Raileanu & Rocktäschel, 2020), focusing on tasks such as:
- DoorKey Random
  - A compact grid environment where the agent must locate a randomly positioned key to unlock a door. Both the agent's starting position and wall configurations change each episode, introducing moderate stochasticity and requiring consistent curiosity-driven exploration.
- MultiRoom-N
  - A sequence of interconnected rooms generated anew each episode, with randomized object placements and door arrangements. This scenario significantly increases environmental randomness, testing our framework's ability to maintain consistent exploration strategies despite considerable state-space variations.
- Noisy-TV Add-On
  - An additional distractor element designed following previous studies (Jarrett et al., 2022; Mavor-Parker et al., 2021). This "noisy TV" tile changes unpredictably upon interaction, providing a clear test for the agent's ability to recognize and discount irreducible random signals during exploration.

We complement these grid-based tasks with two-dimensional maze navigation environments that simulate partial stochasticity:
- Maze Navigation in 2D
  - Inspired by classic navigation benchmarks (Huang et al., 2021; Pathak et al., 2019), this environment consists of randomly generated mazes per episode. Agent movement includes a small probability of random directional shifts, introducing additional stochastic complexity. Agents must navigate these mazes relying primarily on intrinsic rewards, as extrinsic signals are minimal.

We further extend our testing to procedurally generated dungeon tasks based on the MiniHack environment:
- Procedural MiniHack Dungeon
  - A grid-based, continuous-action dungeon featuring varying map topographies, randomized item distributions (including distractor or "noisy" items), and stochastic monster behaviors. The complexity introduced by procedural generation prevents straightforward memorization, thus demanding robust adaptive curiosity-driven exploration (Raileanu & Rocktäschel, 2020).

To examine our framework’s performance in continuous-action domains, we include modified MuJoCo physics-based environments (Peters & Schaal, 2008) with additional partial randomness:
- Swimmer-R
  - A modified Swimmer task where unpredictable random "currents" influence agent movement. Agents must effectively differentiate between controllable and uncontrollable dynamics to maintain forward progression and earn extrinsic rewards.
- HalfCheetah-R
  - Similar to Swimmer-R, the HalfCheetah environment includes random frictional variations or intermittent wind forces. These stochastic elements require agents to identify and adapt robust locomotive patterns, demonstrating the capacity of our curiosity-driven framework to handle high-dimensional continuous tasks effectively (Mavor-Parker et al., 2021).

For all these scenarios, agents face sparse extrinsic rewards, making intrinsic motivation critical for meaningful exploration and learning (Bellemare et al., 2016; Oudeyer & Kaplan, 2007).

Regarding the learning agents (respondents) and their training protocol, we apply well-established RL architectures and algorithms:
- We utilize standard RL architectures such as Proximal Policy Optimization (PPO) or Soft Actor-Critic (SAC), augmented with intrinsic reward signals developed within our curiosity framework. These algorithms efficiently integrate exploration incentives derived from ensemble-based uncertainty estimation, aleatoric uncertainty subtraction, and competence-based skill progression.
- Agents collect trajectories across multiple random seeds in parallel. We maintain careful tuning of hyperparameters, including learning rates and entropy regularization, to ensure stable and reproducible learning outcomes across varying stochastic conditions (Mazzaglia et al., 2021).
- Each agent incorporates a Bayesian ensemble-based approach to quantify epistemic uncertainty, combined with aleatoric uncertainty subtraction to reduce curiosity towards purely random states. Competence-based signals guide agents to seek improvement in meaningful skills or sub-goals, facilitating long-term skill development (Baranes & Oudeyer, 2009; Eysenbach et al., 2018; Gregor et al., 2016).
- To dynamically prioritize among the intrinsic reward components, agents employ an adaptive weighting scheme. This mechanism adjusts based on current policy entropy and visitation frequencies, ensuring balanced exploration that systematically avoids fixation on unlearnable randomness (Huang et al., 2021; Raileanu & Rocktäschel, 2020).

The selected environments and methodological approaches offer a robust and comprehensive testbed to assess the adaptive curiosity framework under various conditions. Our rationale for selecting these tasks stems from the following considerations:
- Including both discrete-grid and continuous-action tasks allows us to verify the generalizability of our curiosity-driven exploration methods across fundamentally different RL representations.
- Each task systematically introduces partial randomness, enabling precise evaluation of how effectively agents identify and discount unlearnable noise signals.
- Sparse external rewards ensure intrinsic motivation mechanisms are crucial, allowing us to directly assess curiosity-driven exploration impacts.
- Our tasks align closely with existing literature (Eysenbach et al., 2018; Mazzaglia et al., 2021; Pathak et al., 2017; Raileanu & Rocktäschel, 2020), facilitating direct comparison with established methods.

In summary, our selection of environments and respondents provides a rigorous experimental foundation. We expect that the adaptive curiosity-driven mechanisms will effectively address exploration challenges posed by partial randomness, significantly improving learning performance and generalization across varied and complex RL scenarios.
