### 3.5 Summary and Implications for Methodology

Previous sections surveyed modern reinforcement learning (RL) concepts relevant to adaptive curiosity in partially random or procedurally generated environments. We traced how knowledge-based (Bellemare et al., 2016; Pathak et al., 2017; Houthooft et al., 2016; Burda, Edwards, Pathak, et al., 2018) and competence-based (Baranes & Oudeyer, 2009; Eysenbach et al., 2018) exploration methods complement one another, with recent multi-signal or hybrid methods (Huang et al., 2021; Raileanu & Rocktäschel, 2020) demonstrating robust exploration even under high stochasticity. We also reviewed the formal unifications of intrinsic motivation—whether modeled through learning progress, prediction error, mutual information, or ensemble disagreement (Jarrett et al., 2022; Achiam et al., 2018). Taken together, these insights inform how to design an experimental approach that probes the balance between epistemic (learnable) and aleatoric (intrinsic randomness) uncertainty (Mavor-Parker et al., 2021).
