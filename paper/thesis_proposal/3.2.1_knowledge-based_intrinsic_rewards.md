#### 3.2.1 Knowledge-Based Intrinsic Rewards

A large family of curiosity methods revolves around estimating **knowledge-based** signals, often quantifying *novelty*, *uncertainty*, or *prediction error* (Oudeyer & Kaplan, 2007). Let:
\[
\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, r, \gamma \rangle
\]
be our Markov Decision Process (MDP), with state space \(\mathcal{S}\), action space \(\mathcal{A}\), transition function \(P(s_{t+1}\mid s_t,a_t)\), extrinsic reward \(r(\cdot,\cdot)\), and discount factor \(\gamma\). An agent produces a policy \(\pi(a_t\mid s_t)\), aiming to maximize expected returns but with *intrinsic* additions to shape exploration.
