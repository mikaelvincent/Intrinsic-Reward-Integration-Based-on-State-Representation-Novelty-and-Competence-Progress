#### 3.5.2 Methodological Implications

To investigate adaptive curiosity in partially random domains, our proposed methodology (detailed in Chapter 4) will integrate the following elements:

- **1. Environment and Procedural Generation**:  
  We will adopt environments with controllable degrees of randomness (Raileanu & Rocktäschel, 2020), ensuring the agent must discern *learnable transitions* from *irreducible noise*. Procedural seeds will vary each episode, preventing memorization.

- **2. Unified Intrinsic Reward**:  
  Inspired by the “hybrid and smoothed” approach (Huang et al., 2021), we plan to incorporate both knowledge-based signals (density or forward-model errors) and competence-based signals (learning progress). Formally, if \(\rho(s)\) denotes a novelty measure and \(\Delta \ell\) denotes progress in a subgoal skill, the agent’s curiosity term may become:
  \[
     r_t^i 
     \;=\;
     \alpha\,[\rho(s_t)]^{-\tfrac12}
     \;+\;
     \gamma\,
     \bigl[\Delta\ell(g)\bigr].
  \]
  The weighting \(\alpha,\gamma\) may adapt based on policy entropy or local state density (Huang et al., 2021).

- **3. Ensemble Uncertainty and Aleatoric Subtraction**:  
  To discount “noisy TV” states, an ensemble-based forward model (Pathak et al., 2019) or an aleatoric uncertainty predictor (Mavor-Parker et al., 2021) can subtract the random component:
  \[
    \hat{r}_t^i 
    \;=\;
    \bigl\|\hat{\mu}_{t+1}-s_{t+1}\bigr\|^2
    \;-\;
    \eta\,\mathrm{Tr}\bigl(\hat{\Sigma}_{t+1}\bigr).
  \]
  This ensures purely random events yield minimal bonus.

- **4. Competence Tracking**:  
  When adopting a competence-based “self-generated goals” approach (Baranes & Oudeyer, 2009), the methodology will partition the goal space and compute \(\Delta(\mathrm{error}_{\mathcal{R}})\) to focus on subregions that demonstrate actual learning gains. This partitioning can be realized via incremental clustering or region-splitting (Baranes & Oudeyer, 2009; see also discussion in R-IAC).

- **5. Implementation and Algorithmic Flow**:  
  - **(A)** Maintain a *replay buffer* for environment transitions.  
  - **(B)** Update *density model* or *ensemble forward model* each iteration to compute knowledge-based bonuses.  
  - **(C)** If using *competence-based*, maintain an error table or local model to track improvement \(\Delta \ell\).  
  - **(D)** Merge final intrinsic reward \(r^i\) with extrinsic \(r^e\).  
  - **(E)** Optimize policy with an off-policy or on-policy RL algorithm (e.g., PPO, SAC).  
  - **(F)** Conduct ablation: examine performance with or without ensemble disagreement, with or without \(\Delta \ell\), and so on.

- **6. Analysis and Metrics**:  
  We plan to measure:
  - **State Coverage** or unique states visited.  
  - **Learning Progress** in a synthetic subtask.  
  - **Episode Return** under partial random seeds.  
  - **Noise Sensitivity**: whether the agent lingers in unlearnable transitions.  

Overall, these guidelines translate the preceding theoretical lessons into a step-by-step methodology for investigating how an agent can effectively learn in partially random or high-dimensional RL settings through a carefully designed *intrinsic reward*, synergy across knowledge-based and competence-based curiosity, and robust discounting of environment noise.
