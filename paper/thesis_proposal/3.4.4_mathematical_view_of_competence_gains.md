#### 3.4.4 Mathematical View of Competence Gains

In many RL formulations, we can unify competence-based rewards with the standard policy gradient objective.

Let \(\mathcal{M}\) be the environment’s MDP, and let \(\mathcal{G}\) be a (potentially learned) goal space. Suppose in each episode, the agent picks a goal \(g\sim p_{\mathrm{goal}}(\cdot)\), then interacts with the environment using policy \(\pi_\theta(\cdot\mid s,g)\). Define:
- Competence Score
  - \[
    \ell_a(g) \;=\; 
    1 \;-\; \mathrm{dist}(s_{\mathrm{final}},g)
    \quad(\text{or any measure of success in achieving }g).
    \]
- Competence Improvement
  - \[
    \Delta \ell_a(g)
    \;=\;
    \ell_a(g)_{\text{new}}
    \;-\;
    \ell_a(g)_{\text{old}}.
    \]
- Intrinsic Reward (averaged over time or tasks)
  - \[
    r^i_t \;=\; \eta\,\Delta \ell_a\bigl(g_t\bigr)
    \]
    or a region-based version \(\Delta(\mathrm{error}_{\mathcal{R}})\). The agent’s total return becomes:
    \[
    J(\theta)
    \;=\;
    \mathbb{E}_{g,\;\tau\sim\pi_\theta(\cdot\mid g)}
    \Bigl[\sum_{t=0}^{T}\,\gamma^t\,\bigl(r^i_t\bigr)\Bigr].
    \]

A gradient-based RL algorithm can then update \(\theta\) to improve policy parameters in directions that yield greater competence progress (Frank et al., 2014; Han et al., 2019). Crucially, once a region saturates—i.e., no further skill gains are possible—the agent’s intrinsic returns there decay, redirecting exploration to other subregions.
