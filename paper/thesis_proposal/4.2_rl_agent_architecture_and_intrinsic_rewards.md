### 4.2 RL Agent Architecture and Intrinsic Rewards

We use **Proximal Policy Optimization (PPO)** as our primary on-policy RL algorithm. For MiniGrid tasks, the policy and value networks each have a small CNN encoder followed by a 2-layer MLP with 256 units, while the Atari domain (Montezuma’s Revenge) uses the standard “Nature DQN” CNN trunk plus a 512-unit fully connected layer for the policy and value heads. MuJoCo tasks (Half Cheetah, Ant Maze) use a 2-layer MLP with 256 hidden units for both policy and value. We run PPO in rollouts of 128 steps for MiniGrid and Atari (or 2048 steps in MuJoCo), then update with 4 minibatch epochs per iteration. We fix the discount factor \(\gamma=0.99\), GAE-\(\lambda=0.95\), learning rate \(3\times10^{-4}\), and clipping parameter 0.2. An entropy coefficient of 0.01 and a value loss coefficient of 0.5 are included.

To address exploration difficulties in partially random or sparse-reward scenarios, we introduce three intrinsic reward signals, combined additively and optionally scaled by dynamic weighting:

1. **Ensemble Disagreement.** We adapt an ensemble-based forward-model approach that draws on ideas from self-supervised disagreement (Pathak et al., 2019). Let \(\{f^{(1)},\dots,f^{(K)}\}\) be an ensemble of \(K\) forward models, each predicting the next state \(\hat{s}_{t+1}^{(k)}\) from \((s_t,\,a_t)\). We define
   \[
   r_{\mathrm{disagree}}(t)
   \;=\;
   \sqrt{
     \frac{1}{K}
     \sum_{k=1}^{K}\bigl(\hat{s}_{t+1}^{(k)} - \mu_{\mathrm{ens}}\bigr)^2
   }
   \quad\text{where}\quad
   \mu_{\mathrm{ens}} \;=\;
   \frac{1}{K}\,\sum_{k=1}^{K}\hat{s}_{t+1}^{(k)}.
   \]
   This “disagreement bonus” is large when the models differ in their predictions, implying epistemic uncertainty about that transition.

2. **Aleatoric Noise Subtraction.** Building on the notion of modeling irreducible environment noise to avoid getting stuck in random transitions (Mavor-Parker et al., 2021), each ensemble member also outputs a diagonal covariance \(\Sigma_{t+1}^{(k)}\). We then reduce the raw forward error by a factor tied to predicted noise. Concretely, let \(\delta_{t+1}^{(k)} = \|s_{t+1}-\hat{s}_{t+1}^{(k)}\|^2\). An example form of the aleatoric-based signal can be
   \[
   r_{\mathrm{aleatoric}}(t)
   \;=\;
   \max_{k}
   \Bigl[\delta_{t+1}^{(k)}
         \;-\;
         \eta\,\mathrm{Tr}\bigl(\Sigma_{t+1}^{(k)}\bigr)\Bigr]_{+},
   \]
   where \(\eta\) is a hyperparameter (often around 0.1) and \(\max(\cdot)_{+}\) sets negative values to 0. If a transition is recognized as predominantly random, the predicted \(\mathrm{Tr}(\Sigma)\) becomes large, reducing this part of the bonus.

3. **Competence-Based Progress.** To incorporate concepts of self-generated goals and improvement (Oudeyer and Kaplan, 2007), we measure the agent’s success on small subgoals (e.g., picking up a key, moving forward a certain distance). At intervals, we record success frequency on these subgoals; the intrinsic bonus is the improvement in success rate. Denoting old and new success ratios by \(\mathrm{successRatio}_{\text{old}}\) and \(\mathrm{successRatio}_{\text{new}}\), respectively, the competence reward is
   \[
   r_{\mathrm{comp}}(t)
   \;=\;
   \mathrm{successRatio}_{\text{new}}
   \;-\;
   \mathrm{successRatio}_{\text{old}}.
   \]
   This encourages the agent to explore subregions of the environment where it is steadily enhancing its mastery of intermediate objectives.

For the final intrinsic bonus, we sum these three signals. In some configurations, we dynamically scale them with factors derived from either (i) the policy’s action-entropy or (ii) a state-distribution measure (such as visitation counts or cluster rarity) or (iii) both. One version unifies them into a single coefficient \(\alpha_j(t)\), while another version uses separate coefficients for the knowledge-based signals (\(r_{\mathrm{disagree}},r_{\mathrm{aleatoric}}\)) vs. the competence-based signal (\(r_{\mathrm{comp}}\)). Either approach aims to amplify exploration when the policy is uncertain or the state is underexplored, then fade out once that region is no longer beneficial.

In practice, after each environment step, we calculate each sub-signal, optionally normalize it with a running mean-std, and then multiply by the relevant dynamic weighting factor(s). This final intrinsic bonus is added to the extrinsic reward for the PPO update. We train the ensemble forward models (and their covariance predictions) in parallel with standard PPO rollouts. For the competence-based subgoals, we track success frequencies over a rolling window of attempts and compute the improvement term. By periodically evaluating with intrinsic bonuses turned off (purely extrinsic), we can compare final task performance and measure how well exploration overcame partial randomness or sparse rewards.
