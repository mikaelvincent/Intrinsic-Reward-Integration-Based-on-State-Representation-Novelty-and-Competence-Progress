#### 1.2.1 General Objective

The general objective of this study is to develop and empirically validate a multi-component adaptive curiosity mechanism for reinforcement learning agents in partially random or procedurally generated domains, wherein the agent can:
- Identify and discount irreducibly noisy aspects of the environment.
- Systematically enhance its internal dynamics model or skill repertoire.
- Achieve robust exploration while avoiding stagnation or infinite loops in random subregions (Baranes & Oudeyer, 2009; Mavor-Parker et al., 2021; Pathak et al., 2019).

By unifying multiple intrinsic motivation signals—such as Bayesian information gain (Houthooft et al., 2016), aleatoric-uncertainty-subtracted prediction error (Mavor-Parker et al., 2021), and competence-based progress (Baranes & Oudeyer, 2009)—the agent is expected to navigate large or procedurally diverse RL tasks without succumbing to the noisy-TV trap or neglecting valuable subregions.
