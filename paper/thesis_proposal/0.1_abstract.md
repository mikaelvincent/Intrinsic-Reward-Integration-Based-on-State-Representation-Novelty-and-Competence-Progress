## Abstract

Recent research in reinforcement learning (RL) has highlighted the importance of intrinsic motivation as a robust means of driving exploration, especially in tasks where extrinsic rewards are sparse, deceptive, or fully absent (Oudeyer & Kaplan, 2007; Bellemare et al., 2016; Gregor et al., 2016; Houthooft et al., 2016; Pathak et al., 2019; Achiam et al., 2018; Eysenbach et al., 2018; Huang et al., 2021). However, many standard curiosity frameworks fail when faced with partially random or highly stochastic dynamics (Mavor-Parker et al., 2021; Mazzaglia et al., 2021). Once random or uncontrollable state transitions produce perpetually high surprise signals, purely novelty-driven agents can become “trapped,” wasting time on irreducible noise (Houthooft et al., 2016; Mavor-Parker et al., 2021).

To address these pitfalls, this study proposes an adaptive curiosity mechanism that unifies three complementary intrinsic rewards:
- Bayesian Ensemble Information Gain (EBIG)
  - Building on ensemble disagreement and Bayesian methods (Houthooft et al., 2016; Pathak et al., 2019), the agent measures how each new observation reduces posterior uncertainty over environment dynamics. Purely random states that cannot reduce uncertainty yield negligible bonuses.
- Aleatoric Noise-Subtracted Prediction Error (Mavor-Parker et al., 2021)
  - Forward-model errors are discounted by the agent’s predicted irreducible noise, preventing “noisy TV” phenomena from producing infinite exploration loops. This ensures curiosity saturates quickly for unlearnable transitions.
- Competence-Based Skill Progress (Baranes & Oudeyer, 2009; Gregor et al., 2016; Eysenbach et al., 2018)
  - By tracking improvements in subgoal mastery or self-generated tasks, the agent continually discovers and refines meaningful behaviors, ignoring transitions that offer no further competence gains.

We integrate these signals with an adaptive weighting scheme (Huang et al., 2021) guided by policy entropy and local visitation frequency. In effect, the agent dynamically prioritizes knowledge-based novelty, noise-aware forward-model exploration, or self-generated goals depending on its current mastery and environment conditions.

Evaluations in procedurally generated or partially random domains (Bellemare et al., 2016; Raileanu & Rocktäschel, 2020) are expected to demonstrate superior exploration coverage and improved task success. By properly discounting irreducible stochasticity (Mavor-Parker et al., 2021) and leveraging skill-discovery submodules (Achiam et al., 2018; Eysenbach et al., 2018), our unified approach aims to achieve robust exploration across a range of challenging environments. Ultimately, this work pushes the boundary of intrinsic motivation research, providing a modular, tri-component framework that adapts effectively to partial randomness while driving sustained skill acquisition and higher-level competence.
