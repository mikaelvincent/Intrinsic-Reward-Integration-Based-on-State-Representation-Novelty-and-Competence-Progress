## Abstract

Recent advancements in reinforcement learning (RL) highlight the importance of intrinsic motivation as an effective method for incentivizing exploration, especially in tasks where extrinsic rewards are sparse, deceptive, or non-existent (Achiam et al., 2018; Bellemare et al., 2016; Eysenbach et al., 2018; Gregor et al., 2016; Houthooft et al., 2016; Huang et al., 2021; Oudeyer & Kaplan, 2007; Pathak et al., 2019). However, standard curiosity frameworks often struggle in environments characterized by partial randomness or high stochasticity (Mavor-Parker et al., 2021; Mazzaglia et al., 2021). When random or uncontrollable state transitions produce continuously high surprise signals, purely novelty-driven agents can become trapped, repeatedly fixating on irreducible noise (Houthooft et al., 2016; Mavor-Parker et al., 2021).

To address these challenges, we propose an adaptive curiosity mechanism that integrates three complementary intrinsic reward signals:
- Bayesian Ensemble Information Gain (EBIG)
  - Building upon ensemble disagreement and Bayesian inference (Houthooft et al., 2016; Pathak et al., 2019), this component rewards agents based on how much new observations reduce posterior uncertainty regarding environment dynamics. Purely random transitions, therefore, receive minimal intrinsic reward.
- Aleatoric Noise-Subtracted Prediction Error
  - This approach discounts forward-model prediction errors by the agent's estimated irreducible noise (Mavor-Parker et al., 2021). Consequently, uncontrollable stochastic transitions (the "noisy TV" effect) produce limited curiosity bonuses, ensuring rapid saturation and preventing endless exploration loops.
- Competence-Based Skill Progress
  - Adopting principles from competence-driven exploration methods (Baranes & Oudeyer, 2009; Eysenbach et al., 2018; Gregor et al., 2016), this signal incentivizes exploration through improvements in accomplishing self-generated tasks or sub-goals.

Additionally, we apply an adaptive weighting strategy (Huang et al., 2021) guided by policy entropy and local visitation frequency. This adaptive weighting strategy dynamically prioritizes among knowledge-based novelty, noise-aware forward-model exploration, and self-generated goal mastery, based on the agent's current proficiency and environmental state.

We expect that empirical evaluations conducted in procedurally generated or partially random environments (Bellemare et al., 2016; Raileanu & Rockt√§schel, 2020) will demonstrate improved exploration efficiency and task performance. Ultimately, we seek to advance intrinsic motivation research by developing an adaptive exploration mechanism capable of effectively handling partial randomness and fostering continuous skill development.
