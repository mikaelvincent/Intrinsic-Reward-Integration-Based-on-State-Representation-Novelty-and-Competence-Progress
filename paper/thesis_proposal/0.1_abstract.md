## Abstract

Reinforcement learning agents frequently struggle to explore in environments that contain partially random transitions and sparse extrinsic feedback. Traditional novelty-driven approaches often overvalue regions of irreducible noise, causing agents to spend disproportionate time on unlearnable transitions. Competence-based methods focus on self-generated goals and improvements in prediction or policy performance, yet they may underemphasize environmental novelty when partial randomness does not interfere. This study proposes an intrinsic reward mechanism that integrates state representation novelty with competence progress to address exploration challenges in partially random domains. By separately tracking irreducibly noisy transitions and genuinely learnable dynamics, the mechanism concentrates exploration on controllable behaviors while discounting random or unproductive subregions. The framework combines a learned embedding network with a forward model, an adaptive region-partitioning scheme, and a competence-based filter to detect learning progress. Experimental evaluations are planned on grid-based and continuous-control tasks, including procedurally generated and large-scale simulators, to assess performance under varying degrees of partial randomness. The intended outcomes include improved sample efficiency, robust policy discovery, and enhanced stability in tasks where naive exploration is hindered by unlearnable transitions. Results will be compared with established curiosity-driven baselines to highlight the benefits of explicitly unifying state representation novelty and competence progress for intrinsic reward. The design is expected to support broad applicability across discrete and continuous benchmarks, potentially informing real-world robotic systems that must contend with sensor noise and uncertain dynamics.
