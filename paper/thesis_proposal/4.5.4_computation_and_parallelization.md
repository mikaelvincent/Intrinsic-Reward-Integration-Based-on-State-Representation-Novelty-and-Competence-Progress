#### 4.5.4 Computation and Parallelization

All experiments are conducted on commodity GPU-based systems or HPC clusters, reflecting standard usage in curiosity-driven RL (Houthooft et al., 2016). The main points:

- Hardware:
  - 1–4 NVIDIA GPUs (e.g., RTX 2080 or similar).
  - 8–64 CPU cores.
  - 32–128 GB RAM, depending on environment complexity and replay buffer usage.
- Parallel Environment Execution:
  - Typically 4–64 environment processes run in parallel, each generating data. The resulting transitions are fed into the RL algorithm’s update queue. This reduces overall training wall-clock time (Burda et al., 2018).
- Model Training and Logging:
  - PyTorch or TensorFlow frameworks are employed.
  - The forward/inverse models or density-based modules train asynchronously or in mini-batches after each PPO iteration.
  - Logging is typically done via standard RL logging frameworks or custom scripts, storing per-update metrics such as policy loss, approximate KL divergences, or curiosity signals (Raileanu and Rocktäschel, 2020).
- Frequency of Updates:
  - On-policy updates occur every few thousand steps. Off-policy or curiosity models (like ICM) often update more frequently, in each mini-batch, to keep the forward model aligned with recent transitions.
- Runtime:
  - A typical environment (like Bipedal Walker or Car Racing) can converge within 24–48 hours of training on a single GPU with moderate parallelization. Harder tasks (like Humanoid) can take multiple days, consistent with high sample budgets (Raileanu and Rocktäschel, 2020).

In summary, the training process leverages parallelism and GPU-accelerated deep learning frameworks to manage the computational demands of curiosity-based RL, particularly for tasks that require millions of steps or complex forward/inverse model computations.
