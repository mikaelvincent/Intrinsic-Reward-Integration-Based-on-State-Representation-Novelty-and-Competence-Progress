#### 3.5.1 Summarizing the Theoretical Lessons

Key theoretical insights include:
- Unified Intrinsic Rewards
  - From a mathematical perspective, recall that an agent’s extrinsic return may be augmented by a curiosity term. If \(r_t\) is the extrinsic reward, an intrinsic component \(r_t^i\) can be added, giving
    \[
      R_{\mathrm{aug}}(s_t,a_t,s_{t+1})
      \;=\;
      r_t \;+\;\beta\;r^i_t,
    \]
    where \(\beta>0\) modulates exploration intensity. Different frameworks define \(r_t^i\) via:
    - State-Novelty (Bellemare et al., 2016; Burda, Edwards, Storkey, & Klimov, 2018),
    - Model Discrepancy (Mavor-Parker et al., 2021; Pathak et al., 2017),
    - Disagreement (Pathak et al., 2019),
    - Information Gain (Houthooft et al., 2016),
    - Competence Progress (Baranes & Oudeyer, 2009),
    - or Impact-Driven changes (Raileanu & Rocktäschel, 2020).
- Partially Random Environments
  - Let the transition kernel be decomposed as
    \[
      P(s_{t+1}\mid s_t,a_t)
      \;=\;
      p_{\theta}(s_{t+1}\mid s_t,a_t)\;\oplus\;\omega(s_t,a_t),
    \]
    indicating that transitions arise partly from learnable structure (\(p_{\theta}\)) and partly from irreducible noise (\(\omega\)). Approaches such as discounting aleatoric uncertainty (Mavor-Parker et al., 2021) or modeling ensemble disagreement (Pathak et al., 2019) are crucial to ensure the agent does not remain stuck in unlearnable regions.
- Self-Generated Goals and Competence
  - Competence-based frameworks revolve around sampling or proposing goals \(g\in\mathcal{G}\) and measuring learning progress \(\Delta \ell(g)\). If \(\ell_{\mathrm{old}}(g)\) and \(\ell_{\mathrm{new}}(g)\) denote the agent’s prior vs. updated skill at achieving \(g\), an intrinsic reward can be
    \[
      r^i \;=\;\ell_{\mathrm{new}}(g) - \ell_{\mathrm{old}}(g).
    \]
    This design naturally rejects purely random transitions (Baranes & Oudeyer, 2009) and fosters mastery of controllable subgoals (Eysenbach et al., 2018).
- Hybrid Multi-Signal
  - Multi-component or unified curiosity signals (Huang et al., 2021) combine state novelty (\(\rho^{-1/2}\) or random embedding errors) with forward-model metrics. Additional weighting schemes (policy entropy, local distribution frequency) can adjust the agent’s exploration pattern over time.
- Hierarchical or Skill-Based
  - Mutual information frameworks (Achiam et al., 2018; Gregor et al., 2016) highlight how learning a latent variable \(z\) that indexes distinct skills fosters coverage and reusability. In partially random settings, skill reliability is favored over ephemeral noise. This synergy between skill discovery and robust curiosity underpins large-scale or open-ended exploration tasks (Jarrett et al., 2022).
