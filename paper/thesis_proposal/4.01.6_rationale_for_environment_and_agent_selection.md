#### 4.01.6 Rationale for Environment and Agent Selection

To anchor our research on robust exploration strategies, we carefully select environments and agents that reflect real-world variations and partial randomness. This choice ensures that our methods are tested across diverse tasks and noise conditions, highlighting both strengths and limitations. Our rationale is guided by several key considerations:
- Varied State and Action Spaces
  - Grid-based tasks test symbolic or discrete action spaces (Raileanu & Rocktäschel, 2020).
  - MuJoCo tasks test continuous high-dimensional control (Peters & Schaal, 2008).
- Partial Randomness Coverage
  - Maze tasks highlight random layout generation and local slippage.
  - Swimmer-R and HalfCheetah-R incorporate time-varying friction or wind.
  - A “noisy TV” mechanism in MiniGrid ensures we can measure how effectively an algorithm avoids indefinite attraction to purely random states (Jarrett et al., 2022).
- Sparse Rewards
  - Each environment or variant has minimal extrinsic signals, thus heavily relying on curiosity-driven exploration.
  - This setup replicates the real-world scenario of discovering crucial goals in partially random domains (Huang et al., 2021).
- Compatibility with Prior Literature
  - Many curiosity-based or skill-discovery approaches have used these or similar tasks (Eysenbach et al., 2018; Mazzaglia et al., 2021; Pathak et al., 2017; Raileanu & Rocktäschel, 2020).
  - Replicating known benchmarks ensures results remain comparable and interpretable (Bellemare et al., 2016).

Thus, the chosen research environments offer a comprehensive testbed. They are sufficiently challenging to stress-test adaptive curiosity under partial randomness (Jarrett et al., 2022; Mavor-Parker et al., 2021), while still being tractable enough to enable extensive experiments and ablations (Baranes & Oudeyer, 2009).
