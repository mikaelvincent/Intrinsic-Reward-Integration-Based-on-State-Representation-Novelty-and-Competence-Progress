#### 3.4.2 Self-Generated Goals and Developmental Flow

An influential example is **R-IAC (Robust Intelligent Adaptive Curiosity)** (Baranes & Oudeyer, 2009). R-IAC partitions the agent’s goal or sensorimotor space into regions, tracks learning progress in each, and dynamically allocates exploration to those yielding nontrivial improvement. This approach can be implemented with various regression or local-model learners—e.g., incremental Gaussian mixture or neural networks—to measure and update local prediction errors. Mathematically:

- Partition the goal space \(\mathcal{G}\) into subregions \(\{\mathcal{R}_1,\dots,\mathcal{R}_M\}\).
- For each region \(\mathcal{R}_j\), store a history of errors \(\{\mathrm{error}_t\}\) from attempts in that region.
- The agent’s *interest* in region \(\mathcal{R}_j\) is \(\mathrm{LP}_j\), the absolute difference in mean error across successive time windows. Formally,
  \[
    \mathrm{LP}_j
    \;=\;\Bigl|\;\langle \mathrm{error}\rangle_{\mathcal{R}_j}^{(t-1)} - \langle \mathrm{error}\rangle_{\mathcal{R}_j}^{(t)}\Bigr|.
  \]
- The agent then stochastically chooses goals from the region with the highest \(\mathrm{LP}_j\), effectively performing *active learning* to maximize competence gains.

Because R-IAC down-weights subregions that are purely random—no incremental progress can be made—and those that are already mastered, it aligns with **adaptive curiosity** in partially random worlds. Empirically, R-IAC outperforms naive or purely random goal selection in complex sensorimotor tasks (Baranes & Oudeyer, 2009).
