##### 4.1.2.3 Impact-Driven Intrinsic Reward

An additional term measures how much the agent's actions change the learned embedding from one step to the next. For each transition $(s_t, a_t, s_{t+1})$, an impact component is computed:
$$
\text{impact}(s_t,a_t,s_{t+1})=
\bigl\|\phi(s_{t+1}) \;-\; \phi(s_t)\bigr\|.
$$

To avoid repetitive toggling, the embedding-based difference is typically scaled or divided by an episodic visitation counter. For instance, the visited state $s_{t+1}$ may be down-weighted each time it reappears within the same episode.
