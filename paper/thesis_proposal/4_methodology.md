## 4. Methodology

This chapter presents the methodological framework for investigating adaptive curiosity mechanisms in partially random reinforcement learning (RL) environments. Drawing on knowledge- and competence-based perspectives (Oudeyer & Kaplan, 2007), we fuse state-novelty and learning-progress ideas (Baranes & Oudeyer, 2009), ensemble-based uncertainty estimations (Houthooft et al., 2016; Pathak et al., 2019), and impact-driven or hybrid intrinsic signals (Bellemare et al., 2016; Raileanu & Rocktäschel, 2020). Such integrative approaches aim to mitigate “noisy TV” pitfalls (Mavor-Parker et al., 2021) and direct exploration toward controllable, learnable transitions in large, procedurally generated or stochastically perturbed tasks. By incorporating both Bayesian-style information gain and competence-based learning progress, this methodology seeks to maintain robust exploration signals while ignoring irreducibly random effects. We further propose adaptive weighting strategies and episodic memory mechanisms to unify multiple intrinsic reward streams (Huang et al., 2021). In the following sections, we detail the research environment, instruments, procedures, analysis, and design approaches that underpin our study, concluding with validation protocols and project management considerations.
