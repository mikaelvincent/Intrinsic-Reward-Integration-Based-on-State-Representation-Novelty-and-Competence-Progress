## 4. Methodology

In this chapter, we outline our methodology for developing and evaluating an adaptive curiosity-driven exploration framework tailored for reinforcement learning (RL) agents in partially random and procedurally generated environments. We detail the environments and data sources chosen to reflect varying degrees of stochasticity, describe the intrinsic reward instruments we employ—including Bayesian ensemble uncertainty estimation, aleatoric noise subtraction, and competence-based progression—and explain our adaptive weighting mechanism for integrating these signals. Finally, we present our procedures for verifying individual components, validating performance through controlled experiments, and thoroughly testing the unified approach against established intrinsic motivation baselines.
