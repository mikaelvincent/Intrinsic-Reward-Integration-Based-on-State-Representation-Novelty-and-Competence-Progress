### 1.1 Rationale of the Study

Reinforcement learning (RL) methods frequently encounter challenges when environments are large, sparse-reward, and partially random (Baranes & Oudeyer, 2009; Bellemare et al., 2016; Mavor-Parker et al., 2021). Traditional exploration strategies, such as naïve random exploration, can be insufficient in these settings (Frank et al., 2014). Various forms of intrinsic motivation have thus emerged, offering internally generated signals that guide exploration by emphasizing novelty, uncertainty, or learning progress (Oudeyer & Kaplan, 2007).

Recent advancements apply such intrinsic motivation to high-dimensional, continuous, or procedurally generated domains (Burda et al., 2018; Houthooft et al., 2016; Raileanu & Rocktäschel, 2020). These techniques address the so-called "noisy TV" effect by discounting regions deemed fundamentally unlearnable or uncontrollable (Baranes & Oudeyer, 2009; Jarrett et al., 2022; Pathak et al., 2019). However, most current methods treat partial randomness only superficially or handle it with specialized environment designs.

The need persists for a unified framework of adaptive curiosity that distinguishes irreducibly random transitions from genuinely learnable dynamics while promoting robust exploration in diverse, partially random RL tasks (Bellemare et al., 2016; Ostrovski et al., 2017). Such an approach would benefit tasks ranging from toy grid-based scenarios to complex continuous-control problems. Integrating recent advances in intrinsic motivation into a coherent methodology can enable stable, efficient exploration where extrinsic feedback is sparse or delayed (Baranes & Oudeyer, 2009; Oudeyer & Kaplan, 2007).

Hence, this study addresses how to design and evaluate an adaptive curiosity mechanism that systematically filters out purely random transitions, yet remains focused on discovering valuable behaviors across diverse RL environments.
