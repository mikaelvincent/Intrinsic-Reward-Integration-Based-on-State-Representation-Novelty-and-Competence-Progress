### 1.1 Rationale of the Study

The quest for robust exploration strategies in reinforcement learning (RL) has intensified as agents move toward challenging domains where external rewards are extremely sparse or partially random (Ladosz et al., 2022; Oudeyer & Kaplan, 2007). Traditional methods such as \(\epsilon\)-greedy or uninformed noise struggle to uncover non-trivial solutions in these environments (Peters & Schaal, 2008; Tsitsiklis & Van Roy, 1997). Consequently, the notion of intrinsic motivation—providing internal reward signals that encourage skill discovery, knowledge acquisition, or novel experience—has emerged as a cornerstone (Bellemare et al., 2016; Oudeyer & Kaplan, 2007).

A wide spectrum of intrinsically motivated approaches have been explored. One major branch, knowledge-based or predictive exploration, emphasizes the mismatch between expected and actual outcomes (Burda, Edwards, Pathak, et al., 2018; Pathak et al., 2017). In these methods, an agent is rewarded for reducing uncertainty about its forward dynamics model (Houthooft et al., 2016; Ostrovski et al., 2017). Notably, variational information maximizing exploration (VIME) (Houthooft et al., 2016) encodes environment dynamics into a Bayesian neural network, driving exploration by each transition’s information gain. Similarly, random network distillation (RND) (Burda, Edwards, Storkey, & Klimov, 2018) uses a static target network and a trainable predictor whose error signals unvisited or unfamiliar states. Meanwhile, ensemble disagreement (Pathak et al., 2019) or Bayesian surprise (Mazzaglia et al., 2021) discount uncontrollable noise once models converge to an average prediction, mitigating the so-called “noisy-TV” trap (Jarrett et al., 2022; Mavor-Parker et al., 2021).

Another prominent family, competence-based or skill-discovery approaches, highlights self-generated goals, skill mastery, and learning progress (Baranes & Oudeyer, 2009; Frank et al., 2014; Han et al., 2019). By focusing on how well an agent masters chosen tasks or subgoals, competence-based methods can systematically ignore random or unlearnable transitions (Baranes & Oudeyer, 2009). Skill-discovery frameworks such as Variational Intrinsic Control (Gregor et al., 2016), DIAYN (Eysenbach et al., 2018), or Variational Option Discovery (Achiam et al., 2018) learn latent-conditioned policies that traverse distinct, reliably attainable states. This competence-driven exploration avoids repeated fixation on ephemeral noise (Groth et al., 2021) and yields reusable sub-policies—particularly beneficial in large or partially random tasks (Pathak et al., 2019).

Despite clear successes, single-signal intrinsic motivation methods often fall short in highly stochastic or procedurally generated environments (Huang et al., 2021; Raileanu & Rocktäschel, 2020). Pure state-novelty (Bellemare et al., 2016; Ostrovski et al., 2017) can diminish rapidly if repeated exact states are rare, whereas pure forward-model errors can remain indefinitely high for uncontrollable dynamics (Mavor-Parker et al., 2021). Hybrid methods propose combining multiple curiosity signals to address these vulnerabilities (Huang et al., 2021; Yuan et al., 2025). For instance, RIDE (Raileanu & Rocktäschel, 2020) focuses on agent-driven state changes; EX2 (Fu et al., 2017) relies on exemplar-based novelty detection; and Deep RL with Hybrid Intrinsic Reward Model (Yuan et al., 2025) merges multiple exploration signals in a single framework. These advanced or unified approaches show promise across partially random or open-ended tasks (Burda, Edwards, Pathak, et al., 2018; Ladosz et al., 2022), where classical methods often fail to adapt.

A further dimension of complexity arises in real-world settings or continuous-control scenarios, where partial randomness can stem from sensor noise, domain shifts, or dynamic multi-factor uncertainties (Frank et al., 2014; Sekar et al., 2020). Bayesian or ensemble-based exploration can robustly discount irreducible noise by converging to uniform predictions on unlearnable state transitions (Houthooft et al., 2016; Pathak et al., 2019). Meanwhile, learning progress or competence-based modules can systematically guide the agent toward subregions where it can still improve (Baranes & Oudeyer, 2009). Yet these advanced frameworks also introduce higher computational overhead and intricate hyperparameter tuning (Groth et al., 2021; Mazzaglia et al., 2021). Balancing coverage, noise avoidance, skill reuse, and sample efficiency remains a key open challenge (Kakade, 2001; Tsitsiklis & Van Roy, 1997).

Given these developments, this study aims to explore adaptive curiosity—a multi-signal, self-driven exploration scheme that systematically handles partial environment randomness while avoiding indefinite loops on stochastic transitions (Jarrett et al., 2022; Mavor-Parker et al., 2021). By fusing knowledge-based signals (e.g., Bayesian ensemble disagreement (Pathak et al., 2019), forward-model error minus predicted aleatoric noise (Mavor-Parker et al., 2021)) with competence-based progress (Baranes & Oudeyer, 2009; Eysenbach et al., 2018), we hypothesize that agents can robustly navigate partially random or procedurally varied domains (Huang et al., 2021; Raileanu & Rocktäschel, 2020). Moreover, incorporating a policy-aware or state-distribution-aware unification (Huang et al., 2021) might dynamically weight each curiosity channel, enabling stable exploration over extended training. Parallel lines of research—like policy gradients (Kakade, 2001; Peters & Schaal, 2008), count-based pseudo-counts (Bellemare et al., 2016; Ostrovski et al., 2017), and skill-discovery (Achiam et al., 2018; Eysenbach et al., 2018; Gregor et al., 2016)—all underscore the possibility that multi-component intrinsic motivation could outperform single-signal baselines, particularly in large RL problems with partial randomness (Ladosz et al., 2022).

Thus, our investigation into adaptive curiosity directly builds upon and unifies key insights from prior knowledge-based, competence-based, and hybrid intrinsic motivation frameworks (Bellemare et al., 2016; Groth et al., 2021; Mavor-Parker et al., 2021; Oudeyer & Kaplan, 2007). By doing so, we hope to push exploration beyond the limitations of purely novelty- or forward-error-driven curiosity, yielding agents that can discount irreducible noise, master relevant sub-tasks, and eventually solve complex, partially random RL benchmarks more efficiently (Baranes & Oudeyer, 2009; Houthooft et al., 2016; Pathak et al., 2017). The following sections detail this study’s methodological approach, bridging state-of-the-art curiosity signals with competence progress modules under a single, adaptive weighting scheme.
