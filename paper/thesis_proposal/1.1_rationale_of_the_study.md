### 1.1 Rationale of the Study

The demand for effective exploration strategies in reinforcement learning (RL) has intensified as agents tackle challenging domains where external rewards are sparse or partially random (Ladosz et al., 2022; Oudeyer & Kaplan, 2007). Traditional methods, such as \(\epsilon\)-greedy or uninformed noise, struggle to discover effective solutions in these settings (Peters & Schaal, 2008; Tsitsiklis & Van Roy, 1997). Consequently, intrinsic motivation—providing internal reward signals that encourage skill discovery, knowledge acquisition, and novel experiences—has become a central concept (Bellemare et al., 2016; Oudeyer & Kaplan, 2007).

Existing intrinsically motivated approaches primarily belong to two categories: knowledge-based and competence-based exploration. Knowledge-based methods reward the agent for reducing uncertainty about its forward dynamics model (Burda, Edwards, Pathak, et al., 2018; Pathak et al., 2017). Variational Information Maximizing Exploration (VIME) (Houthooft et al., 2016) and Random Network Distillation (RND) (Burda, Edwards, Storkey, et al., 2018) exemplify these approaches, incentivizing exploration via Bayesian uncertainty or prediction error. Ensemble disagreement or Bayesian surprise further help mitigate issues like the "noisy-TV" trap, wherein purely random states produce continuous high curiosity signals (Jarrett et al., 2022; Mavor-Parker et al., 2021; Pathak et al., 2019).

In contrast, competence-based or skill-discovery approaches emphasize self-generated goals, skill mastery, and learning progress (Baranes & Oudeyer, 2009; Frank et al., 2014; Han et al., 2019). Methods such as Variational Intrinsic Control (VIC) (Gregor et al., 2016), Diversity is All You Need (DIAYN) (Eysenbach et al., 2018), and Variational Option Discovery (Achiam et al., 2018) foster latent-conditioned policies that systematically pursue attainable states and skills. By measuring learning progress, competence-based methods naturally disregard random or unlearnable transitions, thereby promoting meaningful exploration (Groth et al., 2021).

Despite their strengths, single-signal intrinsic motivation methods often fall short in environments characterized by high stochasticity or procedural generation (Huang et al., 2021; Raileanu & Rocktäschel, 2020). Pure novelty signals rapidly diminish when exact states seldom repeat, while forward-model errors remain perpetually high for uncontrollable dynamics (Mavor-Parker et al., 2021). Recent studies propose combining multiple curiosity signals to address these limitations. Examples include Rewarding Impact-Driven Exploration (RIDE) (Raileanu & Rocktäschel, 2020), Exemplar-based Exploration (EX2) (Fu et al., 2017), and Hybrid Intrinsic Reward models (Yuan et al., 2025). Such hybrid frameworks have shown promise in partially random or open-ended tasks, where classical methods often fail to adapt (Burda, Edwards, Pathak, et al., 2018; Ladosz et al., 2022).

Real-world settings further compound these challenges through sensor noise, domain shifts, and multi-factor uncertainties (Frank et al., 2014; Sekar et al., 2020). Bayesian or ensemble-based exploration robustly discounts irreducible noise, converging predictions on unlearnable transitions (Houthooft et al., 2016; Pathak et al., 2019). Similarly, competence-based exploration systematically guides agents toward regions where improvement is feasible (Baranes & Oudeyer, 2009). However, these advanced approaches introduce higher computational overhead and complex hyperparameter tuning (Groth et al., 2021; Mazzaglia et al., 2021). Balancing exploration coverage, noise avoidance, skill reuse, and sample efficiency remains an open challenge (Kakade, 2001; Tsitsiklis & Van Roy, 1997).
