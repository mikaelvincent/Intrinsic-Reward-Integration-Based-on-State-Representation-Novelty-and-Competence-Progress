### 1.1 Rationale of the Study

Reinforcement learning (RL) methods frequently encounter challenges when environments are large, sparse-reward, and partially random (Baranes and Oudeyer, 2009). Traditional exploration strategies, such as na√Øve random exploration, can be insufficient in these settings (Bellemare et al., 2016). Various forms of intrinsic motivation have thus emerged, offering internally generated signals that guide exploration by emphasizing novelty, uncertainty, or learning progress (Oudeyer and Kaplan, 2007).  

Recent advancements apply such intrinsic motivation to high-dimensional, continuous, or procedurally generated domains (Houthooft et al., 2016; Burda et al., 2018). These techniques mitigate the problem of purely random transitions, also known as the "noisy TV" effect, by discounting regions deemed fundamentally unlearnable or uncontrollable (Baranes and Oudeyer, 2009; Pathak et al., 2019). However, most current methods treat partial randomness superficially or address it only for specific environment designs.  

The need persists for a unified framework of adaptive curiosity that systematically distinguishes truly random transitions from learnable novelty while maintaining robust exploration in diverse, partially random RL tasks (Bellemare et al., 2016; Ostrovski et al., 2017). Such an approach would benefit tasks ranging from toy grid-based scenarios to complex continuous-control problems. Integrating recent advances in intrinsic motivation into a coherent methodology can enable stable, efficient exploration where extrinsic feedback is sparse or delayed (Oudeyer and Kaplan, 2007; Baranes and Oudeyer, 2009).  

Hence, the study addresses how to design and evaluate an adaptive curiosity mechanism that robustly handles partial randomness, discourages unproductive focus on uncontrollable states, and promotes the discovery of valuable behaviors across diverse RL environments.
