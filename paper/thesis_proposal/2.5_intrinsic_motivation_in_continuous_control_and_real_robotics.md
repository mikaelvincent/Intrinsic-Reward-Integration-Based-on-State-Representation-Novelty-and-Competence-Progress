### 2.5 Intrinsic Motivation in Continuous Control and Real Robotics

A longstanding question remains how to adapt these curiosity methods to continuous, complex, or partially random tasks, such as humanoid locomotion or real robot manipulation. Studies using high-performance simulators (Todorov et al., 2012) show that curiosity-driven RL can encourage sophisticated behaviors even with sparse or delayed extrinsic rewards (Groth et al., 2021; Peters & Schaal, 2008). More advanced model-based or planning-based approaches attempt to incorporate learned dynamics into a planning loop, as seen in self-supervised exploration via world models (Sekar et al., 2020). Competence-based frameworks also emphasize an agent setting and achieving self-generated subgoals, which can lead to incremental mastery in partially random but still learnable domains (Baranes & Oudeyer, 2009; Ladosz et al., 2022).

Studies of real-world robotic tasks confirm that action-dependent randomness (Frank et al., 2014) can severely hamper naive novelty methods, unless the agent's intrinsic reward explicitly discards uncontrollable transitions (Mavor-Parker et al., 2021). Ensemble-based disagreement (Pathak et al., 2019) or learning progress signals (Jarrett et al., 2022) may thus guide robust policy learning in physically realistic or sensor-noise-laden environments.
