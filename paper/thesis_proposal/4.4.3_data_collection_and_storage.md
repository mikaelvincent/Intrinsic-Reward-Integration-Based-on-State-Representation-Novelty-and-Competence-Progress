#### 4.4.3 Data Collection and Storage

All runs employ an on-policy scheme with PPO:

- Rollout Generation
  - Parallel environment instances gather transitions: $\{s_t, a_t, r_{\mathrm{ext}}, r_{\mathrm{int}}, s_{t+1}\}$.
  - Each episode terminates at success, failure, or the step limit.
- Intrinsic Reward Modules
  - If using ICM, RND, RIDE, R-IAC, or the proposed method, each step's intrinsic bonus is computed on-the-fly.
  - The total reward $r_{\mathrm{ext}} + \eta \,r_{\mathrm{int}}$ is stored for PPO advantage calculations.
- On-Policy Updates
  - After each batch (e.g., 2048â€“4096 transitions), PPO performs a fixed number of epochs. The policy and, if applicable, the intrinsic module (e.g., forward model) are updated.
- Logging
  - Episode returns (both extrinsic and total) and any relevant module losses (forward model, predictor) are recorded.
  - Each run uses a distinct random seed, which is logged with the environment version for reproducibility.
