#### 2.4.2 Robust Exploration in Stochastic Settings

When environments contain **intrinsic randomness**—for instance, partially random transitions or “noisy TV” distractions—purely novelty- or error-based methods risk infinite loops, repeatedly seeking unlearnable transitions (Mavor-Parker et al., 2021). Research on **aleatoric** and **epistemic** uncertainty has helped mitigate such pitfalls:

- **Information-Gain and Bayesian Approaches**: Variational Information Maximizing Exploration (VIME) (Houthooft et al., 2016) uses a Bayesian neural network to track **posterior** updates after each transition, rewarding the agent for transitions that most reduce **epistemic** uncertainty. Once an environment factor is recognized as irreducibly noisy, VIME’s ensemble no longer finds it “useful,” and the agent shifts exploration elsewhere (Houthooft et al., 2016).  
- **Aleatoric Uncertainty Estimation**: Mavor-Parker et al. (2021) propose subtracting predicted noise from total prediction errors, so purely random states yield minimal intrinsic gain. This approach helps agents “escape stochastic traps,” remaining curious in learnable regions.  
- **Disagreement-Based Bonus**: By training an ensemble of forward models and measuring their variance (Pathak et al., 2019), an agent detects truly learnable transitions while ignoring consistent randomness. Once all models converge to the same uncertain (or average) prediction in random areas, the disagreement bonus falls to near-zero.
