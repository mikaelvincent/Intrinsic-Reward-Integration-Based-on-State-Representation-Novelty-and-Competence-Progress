### 1.2.2 Specific Objectives

To realize this overarching goal, we define the following concrete objectives:
- Intrinsic Reward Design
  - Formulate a three-component intrinsic reward scheme that combines:
    - Knowledge-based signals (e.g., model uncertainty or ensemble disagreement (Pathak et al., 2019; Houthooft et al., 2016)),
    - Noise-aware forward-model corrections (Mavor-Parker et al., 2021),
    - Competence-based or skill-discovery progress (Baranes & Oudeyer, 2009; Gregor et al., 2016; Eysenbach et al., 2018).
- Adaptive Weighting Strategy
  - Derive and implement an adaptive mechanism to regulate how the agent balances these intrinsic signals over time, guided by factors such as policy entropy or local visitation frequencies (Huang et al., 2021; Raileanu & Rocktäschel, 2020).
- Partially Random Environments
  - Evaluate the proposed method on a suite of partially random or procedurally generated tasks (Bellemare et al., 2016; Raileanu & Rocktäschel, 2020), measuring whether the agent consistently avoids irreducible noise and attains better coverage or performance than single-signal baselines.
- Ablation and Analysis
  - Conduct thorough ablation studies removing each component or reverting to static weighting. Analyze the agent’s exploration patterns, coverage metrics, and skill reuse across randomized seeds.
- Empirical Validation
  - Compare to state-of-the-art curiosity-based exploration algorithms (ICM, RND, RIDE, VIME, DIAYN) to assess final task performance, exploration efficiency, and robustness to noise (Pathak et al., 2017; Burda, Edwards, Pathak, et al., 2018; Raileanu & Rocktäschel, 2020; Houthooft et al., 2016; Eysenbach et al., 2018).
