#### 4.4.1 Conceptual Description

The proposed method centers on a forward-modeling scheme that tracks both controllable and random aspects of the environment. By subdividing the state-action space into local "subregions" (inspired by Baranes and Oudeyer, 2009), the agent monitors how prediction accuracy evolves over time within each subregion. This evolution forms the basis of a learning progress metric.

Specifically, each subregion tracks:
- Forward-model error: How well a local predictor forecasts the subsequent state or features.
- Learning progress: The drop in prediction error across multiple updates.

Whenever the agent samples a transition in a subregion, it updates the local prediction model. If the model error in that region decreases significantly, it suggests that states therein are learnable and thus merit further exploration (Oudeyer and Kaplan, 2007). Conversely, if the error stagnates (high variance persisting over many visits), the subregion is deemed partially or entirely random, yielding negligible intrinsic reward. This separation ensures that transitions driven purely by irreducible noise—where improvement is impossible—stop attracting curiosity-based signals (cf. Bellemare et al., 2016; Pathak et al., 2019).

The final exploration bonus combines impact-based signals—measuring how agent actions affect a learned state embedding—with learning-progress signals—identifying regions of maximum improvement potential. Each transition's overall intrinsic reward is accordingly damped or nullified if it corresponds to subregions flagged as unlearnable or excessively random (cf. Raileanu and Rocktäschel, 2020). Such a filtering mechanism preserves exploration capacity in subregions where knowledge gain is still feasible, while ignoring transitions that yield no consistent improvement in predictive accuracy.
