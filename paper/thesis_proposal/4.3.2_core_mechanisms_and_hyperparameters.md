#### 4.3.2 Core Mechanisms and Hyperparameters

Each baseline algorithm uses proximal policy updates (e.g., PPO) or a compatible policy gradient variant as its main learning engine. The differences lie in how their exploration bonus (if any) is computed and integrated with the extrinsic reward. Below is a concise summary of the main mechanisms and typical hyperparameters:

1. **Vanilla PPO (No Intrinsic Reward)**  
   - **Mechanism**: Relies on extrinsic reward signals only; leverages PPO’s clipped objective and generalized advantage estimation (GAE).  
   - **Hyperparameters**:  
     - Learning rate \(\alpha\): typically \(3\times 10^{-4}\) or \(1\times 10^{-3}\).  
     - PPO clip range \(\epsilon\): usually \(0.2\).  
     - GAE parameter \(\lambda\): \(0.95\).  
     - Policy entropy coefficient: set to a small value (e.g., 0.0–0.01) to encourage mild exploration if no intrinsic bonus is used.

2. **ICM (Intrinsic Curiosity Module)**  
   - **Mechanism**: Maintains inverse and forward dynamics models, each a neural network. The agent’s intrinsic reward is the forward-model prediction error in a learned latent state. Each update step trains these models with on-policy or off-policy data.  
   - **Key Hyperparameters**:  
     - Inverse vs. forward loss ratio: often 1.0:1.0.  
     - Embedding dimension: e.g., 256.  
     - Intrinsic reward scaling \(\eta\): e.g., 0.2 or 0.1, to balance extrinsic vs. intrinsic rewards.  
     - Optimizer: commonly Adam with learning rate \(3\times 10^{-4}\).  

3. **RND (Random Network Distillation)**  
   - **Mechanism**: A random CNN (target network) \(\phi_{\text{target}}\) is kept fixed, and a predictor network \(\phi_{\text{pred}}\) learns to distill \(\phi_{\text{target}}(s)\). The intrinsic reward is \(\|\phi_{\text{pred}}(s) - \phi_{\text{target}}(s)\|^2\). This is appended to the extrinsic reward for PPO updates.  
   - **Key Hyperparameters**:  
     - Predictor architecture: typically similar to the target network’s CNN shape (e.g., 3–4 convolutional layers).  
     - Intrinsic reward coefficient \(\eta\): e.g., 0.1 or 0.01, controlling how strongly RND drives exploration.  
     - Observation normalization: states often scaled or standardized to keep reward magnitudes consistent.

4. **RIDE (Rewarding Impact-Driven Exploration)**  
   - **Mechanism**: Learns a representation of states via forward/inverse dynamics. Then the intrinsic reward for step \(t\) is
     \[
       r^{\mathrm{impact}}_t = \frac{\|\phi(s_{t+1}) - \phi(s_t)\|_2}{1 + N_{\mathrm{ep}}(s_{t+1})},
     \]
     where \(N_{\mathrm{ep}}(s_{t+1})\) is the visitation count for \(s_{t+1}\) in the current episode.  
   - **Key Hyperparameters**:  
     - Embedding dimension: typically 128 or 256.  
     - Intrinsic reward coefficient: e.g., 0.1.  
     - Inverse/forward model weighting: e.g., 1:1 ratio in the combined loss.  
     - Episode-level visitation counter reset: ensures repeated toggling yields diminishing returns.

5. **R-IAC (Robust Intelligent Adaptive Curiosity)**  
   - **Mechanism**: Divides the state-action space into local regions (e.g., via a KD-tree or region-splitting approach). Monitors forward-model prediction error in each region. When error decreases (i.e., model improvement), the agent’s “learning progress” is positive, and that region’s intrinsic value is high. Exploration prioritizes subregions with greatest learning progress.  
   - **Key Hyperparameters**:  
     - Region capacity threshold: e.g., 50–200 samples per local region.  
     - Model regression method: a small feed-forward network or local regression to handle forward predictions.  
     - Update frequency: e.g., every 100 steps, re-estimate local learning progress.  
     - Intrinsic reward scaling: e.g., 0.05 or 0.1.  

These hyperparameters are generally environment-dependent. In small-scale discrete tasks, hyperparameters can be set to moderate values (like 0.01–0.1 for intrinsic reward coefficients), while larger continuous tasks often require careful tuning or scaling. All baselines assume Adam or RMSProp optimizers with typical learning rates (\(3\times 10^{-4}\) to \(1\times 10^{-3}\)), gradient norm clipping (e.g., 0.5–5.0), and a batch size in the range of 32–256 for the policy and dynamics models.  

Each baseline’s code is integrated with a standard PPO framework so that the only difference among them is the presence and form of the intrinsic reward module. This ensures a consistent training pipeline while highlighting the effect of each exploration mechanism independently.
