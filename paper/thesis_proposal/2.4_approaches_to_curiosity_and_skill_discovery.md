### 2.4 Approaches to Curiosity and Skill Discovery

Researchers have identified multiple formulations for intrinsic exploration signals. Several unifying themes include:
- Density- or Count-Based: Estimating state-space densities (Bellemare et al., 2016; Ostrovski et al., 2017) or approximating them with hash tables (Tang et al., 2016).
- Forward-Model Error: Learning a predictor for next states (or latent embeddings) and rewarding high prediction error (Pathak et al., 2017).
- Learning Progress: Tracking improvement in prediction accuracy or competence, so that subregions with ongoing improvements receive sustained attention (Baranes & Oudeyer, 2009; Groth et al., 2021).
- Information Gain / Bayesian: Estimating changes in posterior uncertainty about environment dynamics (Houthooft et al., 2016).

In parallel, unsupervised discovery of skills or options has attracted extensive interest. Methods such as Variational Intrinsic Control (Gregor et al., 2016), Diversity Is All You Need (Eysenbach et al., 2018, as cited in Achiam et al., 2018), and Variational Option Discovery (Achiam et al., 2018) learn policies that produce distinguishable outcomes, thereby enabling future tasks to build upon learned sub-behaviors. Additional enhancements like "impact-driven" rewards (Raileanu & Rocktäschel, 2020) or multi-objective approaches (Huang et al., 2021) further unify multiple curiosity signals—state novelty, forward-model error, distribution-based weighting—into a single exploration bonus.
