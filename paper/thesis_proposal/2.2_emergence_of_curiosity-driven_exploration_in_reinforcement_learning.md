### 2.2 Emergence of Curiosity-Driven Exploration in Reinforcement Learning

Reinforcement learning (RL) has grown from early tabular approaches (Tsitsiklis & Van Roy, 1997; Szepesv√°ri, 2010) into powerful deep methods suitable for high-dimensional tasks (Tang et al., 2016). In sparse-reward settings, curiosity-driven exploration has become integral to discovering meaningful policies efficiently (Pathak et al., 2017; Burda et al., 2018). These curiosity strategies typically assign an intrinsic reward when the environment reveals information that updates the agent's internal model (Houthooft et al., 2016). Variational methods show particular promise, such as Variational Information Maximizing Exploration (VIME), which uses a Bayesian neural network to measure information gain about dynamics, thereby discounting transitions that are purely random (Houthooft et al., 2016).

Researchers have also explored modeling disagreement across multiple forward models (Pathak et al., 2019). In this formulation, if an ensemble of learned predictors remains uncertain, the agent obtains higher intrinsic reward, thereby encouraging data collection in uncertain or novel regimes. By contrast, random or uncontrollable transitions lead the ensemble to converge on a stable prediction error, eliminating the infinite exploration loop that naive novelty-based methods often face in partially random regions (Jarrett et al., 2022).
