#### 4.4.1 Hyperparameter Configuration

This section presents the hyperparameter settings used for the PPO algorithm and for both the proposed intrinsic reward approach and the baseline methods. Each hyperparameter is assigned a range of possible values, subject to refinement after initial trial experiments. These refinements ensure that each environment's dynamics, observation spaces, and computational constraints are addressed without prematurely fixing hyperparameters. All hyperparameters will be recorded precisely once the training runs are finalized.

##### 4.4.1.1 PPO Hyperparameters

- Learning Rate (\(\alpha\))
  - Range: \(3\times10^{-4}\) to \(1\times10^{-3}\).
  - Rationale: Lower values (e.g., \(3\times10^{-4}\)) often yield stable convergence, while higher values (e.g., \(1\times10^{-3}\)) can speed up learning but risk instability.
- PPO Clipping Range (\(\epsilon\))
  - Range: 0.1 to 0.2.
  - Rationale: A smaller clipping range can stabilize updates but may slow learning progress; a slightly larger range can accelerate training but increase variance.
- Discount Factor (\(\gamma\))
  - Range: 0.99 to 0.995.
  - Rationale: A slightly higher discount factor encourages longer-horizon credit assignment, which can be beneficial in sparse-reward tasks.
- GAE Parameter (\(\lambda\))
  - Range: 0.90 to 0.95.
  - Rationale: Lower values (0.90) reduce variance of advantage estimates, while higher values (0.95) can better propagate rewards over longer horizons.
- Batch Size
  - Range: 2048 to 4096 transitions per update.
  - Rationale: Larger batches increase stability but also increase wall-clock time per policy update.
- Mini-Batch Size
  - Range: 64 to 256.
  - Rationale: The mini-batch size used during the policy and value network update can be tuned for balance between stable gradient estimates and computational efficiency.
- Entropy Coefficient
  - Range: 0.0 to 0.01.
  - Rationale: May encourage diversity in the policy; set to a smaller value if intrinsic rewards already drive exploration strongly.
- Number of Epochs per Update
  - Range: 3 to 10.
  - Rationale: Repeated minibatch passes can improve data usage but risk policy overfitting to recent rollouts.

##### 4.4.1.2 Intrinsic Reward Approaches

- Proposed Approach
  - Forward Model Learning Rate
    - Range: \(3\times10^{-4}\) to \(1\times10^{-3}\).
    - Rationale: Aligns with the PPO learning rate range to keep the forward model synchronized with changing policy data.
  - Embedding Dimension
    - Range: 128 to 256.
    - Rationale: Larger embeddings capture more state features but can increase computational cost.
  - Learning-Progress Coefficient (\(\alpha_\text{LP}\))
    - Range: 0.05 to 0.2.
    - Rationale: Balances emphasis on subregions where forward-model error decreases over time, ensuring that partially random or unlearnable regions eventually yield near-zero bonus.
  - Impact Measure Coefficient (\(\alpha_\text{impact}\))
    - Range: 0.05 to 0.2.
    - Rationale: Controls how strongly agent-driven state changes (in embedding space) are rewarded, helping avoid repetitive toggling actions.
- ICM (Intrinsic Curiosity Module)
  - Forward vs. Inverse Loss Ratio: Often set to 1:1.
  - Intrinsic Reward Scale: 0.1 to 0.2.
  - Embedding Dimension: 256.
  - Learning Rate: Typically matches the PPO range (\(3\times10^{-4}\) to \(1\times10^{-3}\)).
- RND (Random Network Distillation)
  - Intrinsic Reward Coefficient: 0.01 to 0.1.
  - Predictor Network Learning Rate: \(3\times10^{-4}\).
  - Target Network: Fixed random initialization, no further hyperparameters.
  - Observation Normalization: Possibly turned on to maintain stable reward magnitudes.
- RIDE (Rewarding Impact-Driven Exploration)
  - Embedding Dimension: 128 or 256.
  - Intrinsic Reward Coefficient: 0.05 to 0.1.
  - Episodic State Visitation: Per-episode count used to discount repeated toggling.
  - Forward/Inverse Model Ratio: Typically around 1:1.
- R-IAC (Robust Intelligent Adaptive Curiosity)
  - Region Capacity Threshold: 50â€“200 samples.
  - Learning-Progress-Based Intrinsic Reward: 0.05 to 0.1 scaling.
  - Local Forward Model: Usually a small feedforward regressor with learning rate around \(3\times10^{-4}\).
