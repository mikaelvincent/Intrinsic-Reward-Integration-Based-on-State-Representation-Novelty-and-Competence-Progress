#### 2.1.1 Density Models and Pseudo-Counts
One prominent line of work employs **density models** to track how “familiar” or “novel” states are. In tabular settings, states can be counted directly, but high-dimensional or continuous tasks demand **generalized visitation estimates** through learned probability densities (Bellemare et al., 2016; Ostrovski et al., 2017). Under this framework, a learned model \( \rho_n \) assigns a likelihood to each encountered state \( x \), updated after every step. If the likelihood for \( x \) improves upon re-observation, a so-called **pseudo-count** \(N_n(x)\) can be derived—generalizing classic count-based exploration to large or image-based spaces. The resulting intrinsic reward term \(\propto (N_n(x))^{-1/2}\) encourages exploration of less well-modeled states (Tang et al., 2016). This notion unifies:
- **Count-Based Approaches**: Reward inversely proportional to estimated visitation counts.
- **Information Gain** or **Prediction Gain**: Tied to how much one’s density model learns each time a state recurs.

Such density-based pseudo-counts have been integrated into modern RL algorithms, enabling substantial progress on hard-exploration Atari environments, particularly Montezuma’s Revenge (Bellemare et al., 2016; Ostrovski et al., 2017).
