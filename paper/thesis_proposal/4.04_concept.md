### 4.04 Concept

This section proposes a unified adaptive curiosity framework for exploration in partially random reinforcement learning (RL) environments, drawing on insights from prior intrinsic motivation research (Bellemare et al., 2016; Houthooft et al., 2016; Mavor-Parker et al., 2021), and competence-based exploration strategies (Baranes & Oudeyer, 2009). Our overarching goal is to guide an RL agent to systematically explore learnable aspects of a stochastic environment while discounting unlearnable or purely random transitions. We achieve this by combining three distinct intrinsic reward signals—two grounded in knowledge-based exploration and one in competence-based skill progress—under an adaptive weighting scheme.

<div align="center">

![Conceptual Framework](assets/figures/4.1_conceptual_framework.svg)

</div>
