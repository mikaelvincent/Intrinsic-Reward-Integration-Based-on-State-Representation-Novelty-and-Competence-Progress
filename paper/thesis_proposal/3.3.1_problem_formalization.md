#### 3.3.1 Problem Formalization

A partially random environment may be viewed as a Markov Decision Process \(\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, r, \gamma \rangle\) where the transition kernel is decomposed into:
\[
P(s_{t+1} \mid s_t, a_t)
\;=\;
\underbrace{p_\theta\!\bigl(s_{t+1}\mid s_t,a_t\bigr)}_{\text{learnable dynamics}}
\;\oplus\;
\underbrace{\omega(s_t,a_t)}_{\text{intrinsic randomness}},
\]
with \(\oplus\) symbolically denoting a mixture of deterministic/stochastic processes, and \(\omega\) representing irreducible or “aleatoric” noise (Mavor-Parker et al., 2021). If an agent’s exploration strategy only relies on measuring short-term prediction error over \(\hat{s}_{t+1}\approx p_\theta(s_t,a_t)\), then purely random transitions—i.e. large \(\omega\)—may sustain infinite “novelty.” This produces false exploration incentives, preventing the agent from discovering more learnable subregions.

To illustrate the challenge mathematically, let \(\hat{\Sigma}_{t+1}\) be the model’s predicted variance of the next state:
\[
\hat{\Sigma}_{t+1}
\;=\;
\mathbb{E}\bigl[(s_{t+1}-\hat{\mu}_{t+1}) (s_{t+1}-\hat{\mu}_{t+1})^\top \bigr],
\]
where \(\hat{\mu}_{t+1}\) is the model’s mean estimate. A standard curiosity bonus based on raw squared prediction error \(\|\hat{\mu}_{t+1} - s_{t+1}\|^2\) cannot distinguish epistemic uncertainty (due to limited model knowledge) from aleatoric uncertainty (fundamental environment noise). Once the model learns that transitions in region \(\mathcal{R}\subset\mathcal{S}\) are mostly random, continuing to revisit \(\mathcal{R}\) yields no further “model improvement,” yet raw error remains high. Agents must subtract or discount that noise term to avoid the “noisy TV” trap (Mavor-Parker et al., 2021).
