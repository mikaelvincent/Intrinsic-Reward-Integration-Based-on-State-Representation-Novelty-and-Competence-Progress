#### 3.1.3 Policy Gradient Frameworks

For large or continuous action spaces, policy gradient methods directly parametrize \(\pi_\theta\) (Sutton et al., 2000; Peters & Schaal, 2008). One widely used form is the REINFORCE gradient (Williams, 1992):
\[
\nabla_\theta J(\theta) \;=\; \mathbb{E}\Bigl[\sum_{t=0}^{\infty} \nabla_\theta \log \pi_\theta(a_t \mid s_t)\,G_t \Bigr],
\]
where \(G_t\) is the return from time \(t\). To reduce variance, a baseline (often \(V^\pi\)) is subtracted from \(G_t\). Actor-critic methods unify policy gradients with a learned value function or action-value function (Szepesv√°ri, 2010). The actor updates policy parameters \(\theta\), while the critic updates value parameters \(w\):
\[
\begin{aligned}
&\text{Critic:} \quad w \;\leftarrow\; w \;-\;\alpha_w \,\nabla_w\Bigl[\bigl(R_t + \gamma\,V(s_{t+1};w)-V(s_t;w)\bigr)^2\Bigr],\\
&\text{Actor:} \quad \theta \;\leftarrow\; \theta\;+\;\alpha_\theta\;\nabla_\theta \log \pi_\theta(a_t\mid s_t)\,\bigl(R_t + \gamma\,V(s_{t+1};w)-V(s_t;w)\bigr).
\end{aligned}
\]
(Here \(R_t\) is the immediate reward, and so forth.)
