## 5. Bibliography

Achiam, J., Edwards, H., Amodei, D., & Abbeel, P. (2018). Variational option discovery algorithms. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.1807.10299>

Baranes, A., & Oudeyer, P. (2009). R-IAC: robust intrinsically motivated exploration and active learning. *IEEE Transactions on Autonomous Mental Development*, *1*(3), 155–169. <https://doi.org/10.1109/tamd.2009.2037513>

Bellemare, M. G., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., & Munos, R. (2016). Unifying Count-Based exploration and intrinsic motivation. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.1606.01868>

Burda, Y., Edwards, H., Pathak, D., Storkey, A., Darrell, T., & Efros, A. A. (2018). Large-Scale study of Curiosity-Driven Learning. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.1808.04355>

Burda, Y., Edwards, H., Storkey, A. J., & Klimov, O. (2018). Exploration by random network distillation. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.1810.12894>

Eysenbach, B., Gupta, A., Ibarz, J., & Levine, S. (2018). Diversity is All You Need: Learning Skills without a Reward Function. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.1802.06070>

Frank, M., Leitner, J., Stollenga, M., Förster, A., & Schmidhuber, J. (2014). Curiosity driven reinforcement learning for motion planning on humanoids. *Frontiers in Neurorobotics*, *7*. <https://doi.org/10.3389/fnbot.2013.00025>

Fu, J., Co-Reyes, J. D., & Levine, S. (2017). EX2: Exploration with Exemplar Models for Deep Reinforcement Learning. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.1703.01260>

Gregor, K., Rezende, D. J., & Wierstra, D. (2016). Variational intrinsic control. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.1611.07507>

Groth, O., Wulfmeier, M., Vezzani, G., Dasagi, V., Hertweck, T., Hafner, R., Heess, N., & Riedmiller, M. A. (2021). Is Curiosity All You Need? On the Utility of Emergent Behaviours from Curious Exploration. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.2109.08603>

Han, R., Chen, K., & Tan, C. (2019). Curiosity-Driven Recommendation Strategy for Adaptive Learning via Deep Reinforcement Learning. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.1910.12577>

Houthooft, R., Chen, X., Duan, Y., Schulman, J., De Turck, F., & Abbeel, P. (2016). VIME: Variational Information Maximizing Exploration. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.1605.09674>

Huang, F., Li, W., Cui, J., Fu, Y., & Li, X. (2021). Unified curiosity-Driven learning with smoothed intrinsic reward estimation. *Pattern Recognition*, *123*, 108352. <https://doi.org/10.1016/j.patcog.2021.108352>

Jarrett, D., Tallec, C., Altché, F., Mesnard, T., Munos, R., & Valko, M. (2022). Curiosity in hindsight: intrinsic exploration in stochastic environments. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.2211.10515>

Kakade, S. M. (2001). A natural policy gradient. *Neural Information Processing Systems*, *14*, 1531–1538. <http://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf>

Ladosz, P., Weng, L., Kim, M., & Oh, H. (2022). Exploration in deep reinforcement learning: A survey. *Information Fusion*, *85*, 1–22. <https://doi.org/10.1016/j.inffus.2022.03.003>

Mavor-Parker, A. N., Young, K. A., Barry, C., & Griffin, L. D. (2021). Escaping Stochastic Traps with Aleatoric Mapping Agents. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.2102.04399>

Mazzaglia, P., Catal, O., Verbelen, T., & Dhoedt, B. (2021). Curiosity-Driven exploration via latent Bayesian surprise. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.2104.07495>

Ostrovski, G., Bellemare, M. G., Van Den Oord, A., & Munos, R. (2017). Count-Based Exploration with Neural Density Models. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.1703.01310>

Oudeyer, P., & Kaplan, F. (2007). What is intrinsic motivation? A typology of computational approaches. *Frontiers in Neurorobotics*, *1*. <https://doi.org/10.3389/neuro.12.006.2007>

Pathak, D., Agrawal, P., Efros, A. A., & Darrell, T. (2017). Curiosity-driven exploration by self-supervised prediction. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.1705.05363>

Pathak, D., Gandhi, D., & Gupta, A. (2019). Self-Supervised Exploration via disagreement. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.1906.04161>

Peters, J., & Schaal, S. (2008). Reinforcement learning of motor skills with policy gradients. *Neural Networks*, *21*(4), 682–697. <https://doi.org/10.1016/j.neunet.2008.02.003>

Raileanu, R., & Rocktäschel, T. (2020). RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.2002.12292>

Sekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D., & Pathak, D. (2020). Planning to explore via Self-Supervised world models. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.2005.05960>

Szepesvári, C. (2010). Algorithms for reinforcement learning. In *Synthesis lectures on artificial intelligence and machine learning*. <https://doi.org/10.1007/978-3-031-01551-9>

Tang, H., Houthooft, R., Foote, D., Stooke, A., Chen, X., Duan, Y., Schulman, J., De Turck, F., & Abbeel, P. (2016). #Exploration: A study of Count-Based exploration for deep reinforcement learning. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.1611.04717>

Tsitsiklis, J., & Van Roy, B. (1997). An analysis of temporal-difference learning with function approximation. *IEEE Transactions on Automatic Control*, *42*(5), 674–690. <https://doi.org/10.1109/9.580874>

Yuan, M., Li, B., Jin, X., & Zeng, W. (2025). Deep Reinforcement Learning with Hybrid Intrinsic Reward Model. *arXiv (Cornell University)*. <https://doi.org/10.48550/arxiv.2501.12627>
