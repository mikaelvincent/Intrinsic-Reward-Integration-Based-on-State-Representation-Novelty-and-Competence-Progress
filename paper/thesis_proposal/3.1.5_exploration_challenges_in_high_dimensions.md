#### 3.1.5 Exploration Challenges in High Dimensions

**Exploration** is crucial for RL agents to discover high-reward states, particularly when external rewards are sparse (Bellemare et al., 2016; Pathak et al., 2017). Classic approaches like \(\epsilon\)-greedy or Gaussian noise in actions struggle in large or partially random environments. Several advanced methods incorporate **intrinsic motivation** or **curiosity** (Oudeyer & Kaplan, 2007; Bellemare et al., 2016):

1. **Count-Based Pseudo-Counts**: Extend tabular counting to continuous states via density estimators (Bellemare et al., 2016). The agent obtains a bonus \(\propto \bigl(N_n(s)\bigr)^{-1/2}\), where \(N_n(s)\) is a pseudo-count approximating how frequently states similar to \(s\) have been visited.

2. **Forward-Model Prediction Error**: The agent learns a forward model \(f(s_t,a_t)\approx s_{t+1}\) and uses the prediction error \(\|\hat{s}_{t+1}-s_{t+1}\|\) as intrinsic reward (Pathak et al., 2017). Large errors signal novel transitions.

3. **Ensemble Disagreement**: Train multiple forward models \(f_1,\dots,f_K\), measure disagreement among them as a curiosity bonus (Pathak et al., 2019). True randomness yields consistent average predictions, so disagreement remains low; *learnable* uncertainties maintain high disagreement until resolved.

4. **Learning Progress or Information Gain**: Emphasize how much the agent’s model improves after observing transitions (Oudeyer & Kaplan, 2007; Houthooft et al., 2016). For instance, VIME (Houthooft et al., 2016) uses a Bayesian neural network for dynamics and rewards the KL divergence between the prior and posterior distributions of the model parameters.

Such methods are especially relevant in **partially random** or procedurally generated tasks, preventing the agent from getting stuck in purely noisy states (Jarrett et al., 2022; Mavor-Parker et al., 2021; Raileanu & Rocktäschel, 2020).
