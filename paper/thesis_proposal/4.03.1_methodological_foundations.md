#### 4.03.1 Methodological Foundations

The procedure builds on prior frameworks of knowledge-based, competence-based, and multi-signal intrinsic rewards (Oudeyer & Kaplan, 2007; Baranes & Oudeyer, 2009; Pathak et al., 2019; Eysenbach et al., 2018; Gregor et al., 2016). Specifically:
- Integrative Intrinsic Rewards
  - Aleatoric Noise-Adjusted Error (adapted from Mavor-Parker et al., 2021) subtracts predicted noise from raw prediction errors to discount irreducible randomness.
  - Information Gain via Bayesian Ensembles (Houthooft et al., 2016; Pathak et al., 2019) captures epistemic uncertainty and updates the posterior to reflect how much each transition teaches about environment dynamics.
  - Competence-Based Progress (Baranes & Oudeyer, 2009; Eysenbach et al., 2018) evaluates how well the agent masters self-generated subgoals, awarding improvements in skill reliability.
- Target Environments and Tasks
  - We combine small or moderate tasks (e.g., simple MiniGrid mazes with partial random item placement) with more complex procedurally generated tasks (e.g., selected Procgen levels).
  - Each environment introduces partial or explicit randomness in transitions, initial states, or sub-region layouts.
- Adaptive Weighting Strategy
  - Intrinsic reward signals are blended with dynamic weights that reflect local policy entropy or distribution-based measures, ensuring neither pure novelty nor pure competence dominates (Huang et al., 2021).
  - This procedure lets the agent pivot among signals to avoid getting stuck in random “noisy TV” states (Mavor-Parker et al., 2021) while still achieving broad coverage and skill discovery.
