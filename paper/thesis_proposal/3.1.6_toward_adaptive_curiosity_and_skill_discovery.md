#### 3.1.6 Toward Adaptive Curiosity and Skill Discovery

Beyond *model-free vs. model-based*, a further distinction is **competence-based** exploration, where the agent self-generates goals and measures *competence progress* (Baranes & Oudeyer, 2009; Frank et al., 2014). By focusing on tasks it can learn incrementally, the agent avoids random or unlearnable environment aspects and systematically builds a repertoire of **skills** (Gregor et al., 2016; Eysenbach et al., 2018; Achiam et al., 2018). Such skill-discovery frameworks incorporate:

- **Intrinsic control** or **mutual information** objectives (Gregor et al., 2016) that maximize how distinguishable each skill is by its final states.
- **Diversity is All You Need** (Eysenbach et al., 2018): encourages a set of latent-conditioned policies whose state distributions are mutually discriminable.
- **Variational Option Discovery** (Achiam et al., 2018): training a decoder to reconstruct skill labels from entire trajectories, driving richer exploration and potentially robust sub-policies for partially random or open-ended tasks.

As the field progresses, combining **knowledge-based** (e.g., model uncertainty) and **competence-based** (e.g., skill mastery) approaches stands to produce **adaptive curiosity** that excels at **exploration** in partially random or procedurally generated RL environments (Raileanu & Rockt√§schel, 2020; Huang et al., 2021; Jarrett et al., 2022).
