##### 4.4.1.2 Intrinsic Reward Approaches

- Proposed Approach
  - Forward Model Learning Rate
    - Range: \(3\times10^{-4}\) to \(1\times10^{-3}\).
    - Rationale: Aligns with the PPO learning rate range to keep the forward model synchronized with changing policy data.
  - Embedding Dimension
    - Range: 128 to 256.
    - Rationale: Larger embeddings capture more state features but can increase computational cost.
  - Learning-Progress Coefficient (\(\alpha_\text{LP}\))
    - Range: 0.05 to 0.2.
    - Rationale: Balances emphasis on subregions where forward-model error decreases over time, ensuring that partially random or unlearnable regions eventually yield near-zero bonus.
  - Impact Measure Coefficient (\(\alpha_\text{impact}\))
    - Range: 0.05 to 0.2.
    - Rationale: Controls how strongly agent-driven state changes (in embedding space) are rewarded, helping avoid repetitive toggling actions.
- ICM (Intrinsic Curiosity Module)
  - Forward vs. Inverse Loss Ratio: Often set to 1:1.
  - Intrinsic Reward Scale: 0.1 to 0.2.
  - Embedding Dimension: 256.
  - Learning Rate: Typically matches the PPO range (\(3\times10^{-4}\) to \(1\times10^{-3}\)).
- RND (Random Network Distillation)
  - Intrinsic Reward Coefficient: 0.01 to 0.1.
  - Predictor Network Learning Rate: \(3\times10^{-4}\).
  - Target Network: Fixed random initialization, no further hyperparameters.
  - Observation Normalization: Possibly turned on to maintain stable reward magnitudes.
- RIDE (Rewarding Impact-Driven Exploration)
  - Embedding Dimension: 128 or 256.
  - Intrinsic Reward Coefficient: 0.05 to 0.1.
  - Episodic State Visitation: Per-episode count used to discount repeated toggling.
  - Forward/Inverse Model Ratio: Typically around 1:1.
- R-IAC (Robust Intelligent Adaptive Curiosity)
  - Region Capacity Threshold: 50â€“200 samples.
  - Learning-Progress-Based Intrinsic Reward: 0.05 to 0.1 scaling.
  - Local Forward Model: Usually a small feedforward regressor with learning rate around \(3\times10^{-4}\).
