### 2.6 Summary of Findings and Open Challenges

A broad spectrum of knowledge-based and competence-based exploration strategies has emerged over the past decade, shedding light on how agents can learn efficiently in environments with sparse external feedback or partial randomness (Oudeyer & Kaplan, 2007). Knowledge-based approaches, particularly those grounded in density estimation or prediction error, have offered key insights for incentivizing novel state discovery (Bellemare et al., 2016; Tang et al., 2016; Ostrovski et al., 2017). Some rely on pseudo-counts derived from learned probability models (Bellemare et al., 2016; Ostrovski et al., 2017), while others adopt forward dynamics (Pathak et al., 2017) or random network distillation (Burda, Edwards, Storkey, & Klimov, 2018; Burda, Edwards, Pathak, et al., 2018). Bayesian or information-gain methods, notably VIME (Houthooft et al., 2016), seek to reduce epistemic uncertainty over environment dynamics, favoring transitions that refine the agent’s internal model.

Concurrently, competence-based methods emphasize self-generated goals and learning progress in accomplishing them (Baranes & Oudeyer, 2009; Frank et al., 2014; Han et al., 2019). By focusing on how well the agent masters sub-tasks or skills, these approaches naturally discount irreducibly random aspects of an environment—once a task proves unlearnable or yields no further improvement, exploration shifts elsewhere (Baranes & Oudeyer, 2009). In partially stochastic domains, such competence-driven frameworks can avoid the “noisy TV” pitfall by emphasizing regions of the environment that yield genuine gains in mastery (Jarrett et al., 2022; Mavor-Parker et al., 2021).

A number of hybrid and multi-signal formulations aim to balance the strengths of each approach (Oudeyer & Kaplan, 2007; Huang et al., 2021; Yuan et al., 2025). Some unify state-novelty, forward-model error, and ensemble disagreement (Mazzaglia et al., 2021) under a single scheme, while others dynamically fuse policy-centric or distribution-centric rewards (Huang et al., 2021). Modern frameworks such as RIDE (Raileanu & Rocktäschel, 2020) emphasize impact-driven exploration, where agent-controllable changes in environment states yield a robust signal even under extensive partial randomness. In parallel, Plan2Explore (Sekar et al., 2020) highlights self-supervised world models combined with disagreement-based planning, and Curiosity in Hindsight (Jarrett et al., 2022) refines standard predictive bonuses to filter out irreducible noise. Self-supervised exploration methods that learn multiple skills or “options”—such as DIAYN (Eysenbach et al., 2018), VIC (Gregor et al., 2016), and VALOR (Achiam et al., 2018)—have shown how diverse, latent-conditioned policies can emerge in an open-ended manner, though partial randomness still poses open challenges for reliably distinguishing controllable transitions from noise (Groth et al., 2021).

Overall, these investigations confirm that intrinsic motivation can drive exploration in high-dimensional or procedurally generated domains, prompting agents to uncover a suite of robust behaviors even when extrinsic rewards are sparse or absent (Burda, Edwards, Pathak, et al., 2018; Ladosz et al., 2022). Yet the literature also reveals persistent open challenges such as:
- Discounting Purely Random Transitions
  - While advances in ensemble disagreement (Pathak et al., 2019; Mavor-Parker et al., 2021), latent Bayesian surprise (Mazzaglia et al., 2021), or impact-driven objectives (Raileanu & Rocktäschel, 2020) mitigate the “noisy TV” effect, reliably recognizing and ignoring irreducibly stochastic states remains an unsolved problem, especially in complex, real-world contexts (Jarrett et al., 2022).
- Long-Horizon and Hierarchical Exploration
  - Many current methods focus on short-term novelty or competence gains. Complex tasks with partial observability and multi-step dependencies may demand hierarchical skill discovery (Achiam et al., 2018; Eysenbach et al., 2018) and stable memory structures to track long-range goals (Groth et al., 2021). Ensuring robust skill reuse in randomly generated layouts or open-ended scenarios still poses significant challenges.
- Scalability and Computational Costs
  - Approaches such as Bayesian neural networks (Houthooft et al., 2016), large ensembles (Pathak et al., 2019), or advanced density estimators (Bellemare et al., 2016; Ostrovski et al., 2017) can be computationally expensive. Future work may require more lightweight surrogates or specialized architectures (Huang et al., 2021; Yuan et al., 2025).
- Balancing Multiple Intrinsic Signals
  - Unified or hybrid reward methods (Huang et al., 2021) risk complex interactions among curiosity signals. Dynamically adjusting weights or scheduling different exploration modes may yield greater robustness across partially random or non-stationary tasks (Mazzaglia et al., 2021; Yuan et al., 2025).
- Translational Gaps to Real-World Robotics
  - Though curiosity has been applied to humanoid motion planning (Frank et al., 2014) and manipulation, bridging the gap from simulation to physical systems remains an active research area, with partial randomness (e.g., sensor noise, environment changes) amplifying difficulty.

In conclusion, numerous intrinsic motivation formulations—ranging from prediction error and information gain to competence progress—have proven effective for exploration under sparse rewards. Yet no single approach universally solves the challenge of partially random or open-ended RL tasks. Ongoing research thus centers on fusing or adaptively weighting multiple curiosity signals, designing scalable model-based or model-free frameworks, and applying these methods in dynamic, real-world settings where partially stochastic factors are ubiquitous. 
