#### 3.2.4 Hybrid Intrinsic Rewards and Multi-Signal Approaches

Single-signal curiosity often fails in complex or partially random domains. Consequently, multi-signal or unified frameworks fuse multiple novelty measures (Huang et al., 2021). For instance:
\[
r^i_t \;=\; \alpha\,r_{\text{state-novelty}}(s_{t+1})
 \;+\; \beta\,r_{\text{forward-error}}(s_t,a_t,s_{t+1})
 \;+\; \dots
\]
with dynamic weighting \(\alpha,\beta\) factoring in policy entropy or local state distributions. Another strategy (Raileanu & Rockt√§schel, 2020) focuses on impact-driven exploration, e.g.,
\[
r^i_t \;=\;\bigl\|\phi(s_{t+1}) - \phi(s_t)\bigr\|
\]
discounted within an episode to avoid toggling the same controllable states infinitely. By combining coverage-based novelty, forward-model error, or ensemble disagreement, the agent remains robust to random distractors while systematically exploring learnable subregions (Burda, Edwards, Pathak, et al., 2018).
