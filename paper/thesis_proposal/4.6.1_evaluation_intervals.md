#### 4.6.1 Evaluation Intervals

The chosen frequency and procedure for model evaluation play a crucial role in tracking progress and mitigating potential overfitting to the training dynamics (Bellemare et al., 2016). This section clarifies how often the policy is evaluated, whether intrinsic rewards are deactivated during test rollouts, and the number of episodes used per evaluation.

1. Frequency of Evaluation
   - The agent is periodically paused at fixed training intervals (e.g., every 50k or 100k environment timesteps). This consistent schedule aligns with common practice in curiosity-driven learning (Pathak et al., 2017; Burda et al., 2018).
   - Each interval's performance is recorded and added to a log, enabling the plotting of learning curves over time.
2. Rollout Setup
   - Each evaluation typically runs for a fixed number of episodes \(N_{\mathrm{eval}}\) (e.g., 10 or 20 episodes per evaluation).
   - Intrinsic rewards are either set to zero or excluded from the agent's action selection during evaluation, ensuring that the final performance captures extrinsic success without additional exploration bias (Houthooft et al., 2016).
   - If the environment is large or partially random, the agent is evaluated across multiple seeds during each interval to reduce variance in the performance estimate.
3. Non-Episodic and Episodic Environments
   - For non-episodic domains (e.g., ongoing tasks with no terminal state), an artificial maximum horizon is imposed to measure progress at consistent intervals.
   - For episodic domains, each rollout ends upon environment termination or when a maximum step limit is reached, as defined in the environment specifications (see Section 4.2).

These standard intervals and consistent rollouts ensure that short-term fluctuations do not obscure underlying trends, thereby providing a reliable view of policy improvement (Raileanu and Rockt√§schel, 2020).
