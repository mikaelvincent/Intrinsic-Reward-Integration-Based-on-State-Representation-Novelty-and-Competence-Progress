##### 3.2.1.2 Forward-Model Errors and Prediction-Based Curiosity
A widespread alternative is to maintain a forward model \(f_\theta\) that predicts \(s_{t+1}\) from \((s_t,a_t)\). The agent’s intrinsic reward often takes the form (Pathak et al., 2017):
\[
r^i_t \;=\;\bigl\|\hat{s}_{t+1} - s_{t+1}\bigr\|^2,
\]
where \(\hat{s}_{t+1}=f_\theta(s_t,a_t)\) is the predicted next state or embedding. Larger *prediction error* signals novel or underexplored transitions. However, purely local errors can be “fooled” by partial randomness (the “noisy TV” effect). Methods like **Random Network Distillation (RND)** (Burda, Edwards, Storkey, & Klimov, 2018) fix a *target network* \(f(\cdot)\) and train a *predictor* \(g_\phi(\cdot)\) to match \(f\). The curiosity is \(\|g_\phi(s)-f(s)\|^2\). Because the target network is static and *deterministic*, random noise eventually yields minimal disagreement, circumventing indefinite reward attraction to unlearnable transitions.
