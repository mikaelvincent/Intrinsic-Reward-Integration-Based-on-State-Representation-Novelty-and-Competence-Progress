##### 3.2.1.1 Pseudo-Count Methods

Building on tabular RL’s count-based exploration, pseudo-count formulations track density estimates \(\rho_n(x)\) for states \(x \in \mathcal{S}\). As the agent updates \(\rho_n\) after each new sample, it derives a pseudo-count \(N_n(x)\) (Bellemare et al., 2016). In one approach:
\[
N_n(x) \;=\; \frac{\rho_n(x)\,\bigl(1 - \rho_n^+(x)\bigr)}{\rho_n^+(x) - \rho_n(x)},
\]
where \(\rho_n^+(x)\) is the probability assigned to \(x\) after it is observed again, the agent receives a bonus
\[
r^i_t \;\propto\;\bigl(N_n(s_{t+1})\bigr)^{-\tfrac12},
\]
aligning with classical \(\tfrac{1}{\sqrt{N}}\) count-based rewards. This mechanism generalizes well to partially random domains if the density model recognizes irreducibly noisy states, eventually assigning them high predicted probability (Ostrovski et al., 2017). By discounting “pure noise,” pseudo-count approaches encourage meaningful exploration in learnable regions (Tang et al., 2016).
