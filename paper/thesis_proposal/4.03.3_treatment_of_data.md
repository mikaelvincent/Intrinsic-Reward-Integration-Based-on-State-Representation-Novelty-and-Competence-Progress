#### 4.03.3 Treatment of Data

Once the raw data is collected, the subsequent data treatment steps involve model updates, reward generation, and analytic processing:
- Offline and Online Model Updates
  - Forward Models and Bayesian Inference
    - We approximate the posterior over neural network weights for the environment dynamics. For each new batch \((s_t, a_t, s_{t+1})\) from the replay buffer, we update the ensemble’s distribution parameters (Pathak et al., 2019; Houthooft et al., 2016).
    - We measure the KL divergence between old and new distributions to compute the information gain–based curiosity component (Houthooft et al., 2016).
  - Aleatoric Uncertainty Estimation
    - Each forward model predicts both a mean \(\hat{\mu}_{t+1}\) and covariance \(\hat{\Sigma}_{t+1}\). We incorporate that in the intrinsic reward as \(\|\hat{\mu}_{t+1} - s_{t+1}\|^2 - \eta\,\mathrm{Tr}(\hat{\Sigma}_{t+1})\) to discount random transitions (Mavor-Parker et al., 2021).
- Intrinsic Reward Synthesis
  - We combine these knowledge-based signals with a competence-based progress term:
    - Competence Gains: \(\Delta(\text{error region})\) or \(\ell_{\mathrm{new}}(g_t)-\ell_{\mathrm{old}}(g_t)\) if using self-generated subgoals (Baranes & Oudeyer, 2009; Eysenbach et al., 2018). 
  - The final intrinsic reward each step is:
    \[
      r^i_t 
      = w_{\mathrm{IG}} \,\Bigl[\mathrm{KL}\bigl(q_{\mathrm{new}}\|\;q_{\mathrm{old}}\bigr)\Bigr]
      \;+\; w_{\mathrm{Aleatoric}} \,\bigl(\|\hat{\mu}_{t+1}-s_{t+1}\|^2 - \eta\,\mathrm{Tr}(\hat{\Sigma}_{t+1})\bigr)
      \;+\; w_{\mathrm{Comp}}\,\Delta(\mathrm{competence}).
    \]
  - We normalize each component and apply smoothing or an adaptive weighting scheme (Huang et al., 2021).
- Policy Learning and Logging
  - The combined reward \(\bigl[r^e_t + r^i_t\bigr]\) is fed into a standard RL algorithm (e.g., PPO or SAC). 
  - We log:
    - Extrinsic returns,
    - Intrinsic returns (disaggregated by each signal),
    - Episode length, coverage metrics, skill usage frequency,
    - Computed partial random seeds or transitions recognized as uncontrollable noise,
    - Weighted sums \(\bigl(w_{\mathrm{IG}}, w_{\mathrm{Aleatoric}}, w_{\mathrm{Comp}}\bigr)\) over training time.
- Analysis Pipelines:
  - Coverage and Heatmaps
    - We group visited states by environment seed or layout. We plot 2D or 3D heatmaps if the environment is low-dimensional (MiniGrid). 
   - Exploration Efficiency
     - Evaluate how many steps are required to reach key subgoals or to solve the puzzle. 
   - Competence vs. Noise
     - We plot how often the agent lingers in random subregions or how quickly it shifts to more learnable transitions.
- Performance Comparisons and Ablations:
  - We systematically remove or replace each intrinsic component (e.g., removing the aleatoric subtraction) and measure performance drops. 
  - We also compare to single-signal baselines from prior works (Pathak et al., 2019; Eysenbach et al., 2018; Gregor et al., 2016) to quantify the net effect of unifying multiple curiosity signals.

By carefully applying these treatment steps, we obtain a holistic view of how adaptive curiosity influences exploration, how partial randomness is handled, and which components are most critical for robust performance in large or procedurally generated RL tasks.
