## 3. Technical Background

Research on **adaptive curiosity** in partially random reinforcement learning (RL) environments has drawn extensively on intrinsic motivation concepts (Oudeyer & Kaplan, 2007). Early methods explored **knowledge-based** signals such as **pseudo-counts** (Bellemare et al., 2016) or **forward-model error** (Pathak et al., 2017), enabling agents to seek out novel or surprising transitions. Meanwhile, **Bayesian neural approaches** (Houthooft et al., 2016) introduced the idea of maximizing *information gain* about environment dynamics, mitigating over-exploration of irreducible noise (Mavor-Parker et al., 2021). In parallel, **competence-based** methods considered an agentâ€™s self-generated goals and progress in mastering them, further advancing robust exploration in stochastic or procedurally generated worlds (Gregor et al., 2016; Oudeyer & Kaplan, 2007). These frameworks shape how an agent adapts its exploration strategy, focusing on genuinely learnable transitions while discounting purely random events. Consequently, a rich body of methods has emerged to address the dual challenge of sparse extrinsic rewards and partial randomness, laying the groundwork for adaptive curiosity approaches that can discover and retain meaningful skills in open-ended tasks.
