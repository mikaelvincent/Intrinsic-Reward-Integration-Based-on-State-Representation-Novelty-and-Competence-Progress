#### 2.1.4 Unifying Perspectives and Challenges

Overall, knowledge-based and predictive exploration methods share the theme of rewarding model improvement, whether by:
- Estimating state density or pseudo-counts (Bellemare et al., 2016; Ostrovski et al., 2017; Tang et al., 2016).
- Using forward-model prediction error as a novelty signal (Pathak et al., 2017; Burda, Edwards, Pathak, et al., 2018).
- Tracking Bayesian information gain in learned dynamics models (Houthooft et al., 2016).
- Combining or adapting these signals to discount unlearnable stochastic transitions (Mavor-Parker et al., 2021; Pathak et al., 2019).

Major open issues include:
- Noisy Environments
  - Designing intrinsic rewards that ignore purely random events yet still reward discoverable surprises remains an active research topic (Jarrett et al., 2022; Huang et al., 2021).
- Long-Horizon Dependencies
  - Purely local prediction error can miss multi-step tasks requiring strategic exploration. Hybrid approaches combining high-level goals, competence-based objectives, or hierarchical skill discovery (Baranes & Oudeyer, 2009; Gregor et al., 2016; Achiam et al., 2018) may help address such complexity.
- Scalability and Practical Implementation
  - Methods like RND or pseudo-count require careful normalization or architecture choices, while Bayesian networks (VIME) impose heavier computational overhead. Identifying robust, efficient approaches across diverse RL tasks remains an ongoing challenge.

Many of these strategies have exhibited success in partially random or procedurally generated environments (Raileanu & Rockt√§schel, 2020). By combining knowledge-based signals with environment-agnostic representations and thorough uncertainty handling, these methods continue to push the boundaries of sample-efficient exploration in complex RL scenarios.
