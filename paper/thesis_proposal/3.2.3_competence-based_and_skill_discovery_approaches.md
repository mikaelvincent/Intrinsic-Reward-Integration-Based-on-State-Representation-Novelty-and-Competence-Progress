#### 3.2.3 Competence-Based and Skill Discovery Approaches

An alternative to purely knowledge-based signals is to reward how the agent *improves* in achieving *self-generated goals* (Oudeyer & Kaplan, 2007). Key examples:

- **Learning Progress**: R-IAC (Baranes & Oudeyer, 2009) partitions state/action space into regions, awarding *progress* in reducing prediction or control errors. It systematically avoids unlearnable noise, focusing on “just-right” challenges. Formally,
  \[
    r^i_t \;\approx\;\Delta\Bigl(\text{error region}\Bigr)
    \;=\;\Bigl(\mathrm{error}^{\text{old}}_{\text{region}}
             -\mathrm{error}^{\text{new}}_{\text{region}}\Bigr).
  \]
- **Skill Discovery**: The agent learns latent-conditioned policies \(\pi_\theta(a \mid s,z)\) that produce reliably distinguishable *outcomes*. Methods such as:
  1. **Variational Intrinsic Control (VIC)** (Gregor et al., 2016) maximize
     \(\;I(z; s_{\text{final}}) \;=\; H(z) - H\bigl(z\mid s_{\text{final}}\bigr)\).
  2. **DIAYN** (Eysenbach et al., 2018) encourage mutual information between the entire state visitation distribution and the skill index \(z\).  
  3. **VALOR** (Achiam et al., 2018) decode skill labels from full trajectories.

Under partial randomness, these methods discount uncontrollable transitions, since *competence progress* or skill reliability is unattainable in purely random contexts (Jarrett et al., 2022).
