#### 4.6.5 Computation Time and Sample Efficiency

Finally, consistent comparisons of training time and sample efficiency help contextualize each method's practical viability (Bellemare et al., 2016; Burda et al., 2018).

The following protocols address differences in computational complexity and the total environment steps needed to reach certain thresholds:
1. Wall-Clock Time
   - Start and end timestamps are recorded for each run, discounting any paused intervals or system overhead.
   - Summaries include total training hours and an average frame (or timestep) processing rate, allowing fair cross-algorithm comparisons (Raileanu and Rockt√§schel, 2020).
2. Sample Efficiency
   - The main measure is the total number of environment steps (or episodes) until the agent achieves a target performance level (Houthooft et al., 2016).
   - If an algorithm consistently solves tasks in fewer steps, it is deemed more sample-efficient. This metric is especially relevant in partially random tasks where naive exploration can waste a large amount of interaction.
3. Hardware Configuration
   - Because some methods rely on parallelization or GPU-accelerated model training, specifying CPU/GPU type and quantity clarifies whether results can be replicated or scaled (Bellemare et al., 2016).
4. Method Complexity
   - Algorithms with ensemble-based or forward-model-based curiosity (Pathak et al., 2019) might incur higher memory usage or training overhead.
   - If the environment or approach demands large replay buffers or multi-actor sampling (Burda et al., 2018), the overall training time can vary. Reporting the average or maximum resource consumption is recommended.
5. Trade-Off Analysis
   - Methods that achieve excellent exploration might do so at the cost of significant extra computation (Baranes and Oudeyer, 2009). Understanding whether the performance gain justifies that cost is essential for real-world applications.
   - For instance, the proposed approach might scale better in tasks with partial randomness, but verifying that overhead remains manageable is key to feasibility in larger environments or robotics setups.

By reporting both the environmental steps-to-convergence and the total runtime, these metrics characterize each approach's real-world applicability and highlight trade-offs between advanced exploration strategies and computational demands.
