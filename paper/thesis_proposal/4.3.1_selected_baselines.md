#### 4.3.1 Selected Baselines

1. Vanilla PPO
   - A proximal policy optimization (PPO) agent without any intrinsic reward signal. This serves as a control baseline to measure the effect of curiosity-based exploration. The PPO algorithm optimizes a clipped objective to stabilize policy gradient updates. It relies solely on extrinsic rewards, if available, or on random exploration if extrinsic rewards are sparse or absent.
2. ICM (Intrinsic Curiosity Module)
   - A method introduced in (Pathak et al., 2017). It augments a PPO agent with an additional curiosity reward computed from a learned forward and inverse dynamics model. The agent's intrinsic reward is the prediction error of the forward model, which attempts to predict the next latent state from the current latent state and action. The latent state is learned via the inverse model that predicts actions from consecutive states. ICM encourages the agent to seek transitions that are hard to predict, thus driving exploration in state spaces with sparse extrinsic rewards.
3. RND (Random Network Distillation)
   - Proposed by (Burda et al., 2018). This method includes a randomly initialized target network and a predictor network trained to match the target's outputs. The intrinsic reward is the mean-squared error between the predictor and the (fixed) target-network features of the visited states; high error indicates a novel or less-visited state. Because the target network remains fixed, the agent's exploration gradually reduces this error for states it frequently visits. RND is known to be effective in large-scale image-based tasks, provided states are somewhat repeatable.
4. RIDE (Rewarding Impact-Driven Exploration)
   - Proposed by (Raileanu & Rocktäschel, 2020). This method measures how much the agent's actions change a learned state embedding. The intrinsic reward for each transition is the \(\ell_2\) distance in embedding space between consecutive states, discounted by an episodic visitation counter. As a result, RIDE emphasizes environment-impacting actions (like opening doors or picking items) while reducing the bonus when toggling the same states repeatedly. It has shown strong performance in procedurally generated tasks with sparse rewards.
5. R-IAC (Robust Intelligent Adaptive Curiosity)
   - Introduced in (Baranes & Oudeyer, 2009). This method partitions the state-action space into subregions and monitors the agent's learning progress—namely, how much the prediction error in each subregion decreases over time. R-IAC focuses exploration on subregions that offer the greatest potential for further reduction in error. It is designed to avoid fully random or unlearnable subregions and trivial regions where predictions are already near perfect.
