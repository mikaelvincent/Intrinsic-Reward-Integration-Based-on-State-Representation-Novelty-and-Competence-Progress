### 1.3 Significance of the Study

Environments containing partially random transitions often cause exploration agents to either overlook important, learnable regions or become trapped by irreducibly noisy states, a shortcoming highlighted by prior intrinsic motivation and exploration methods (Bellemare et al., 2016; Pathak et al., 2019). Identifying and discarding unlearnable transitions is therefore critical for stable exploration, as ignoring such stochastic variations prevents infinite loops in so-called "noisy TV" scenarios (Pathak et al., 2017; Raileanu and Rockt√§schel, 2020). The framework proposed in this study addresses this challenge by systematically distinguishing controllable novelty from intrinsic randomness, thereby enabling robust exploration across diverse continuous or discrete domains. Such a mechanism unifies earlier concepts of novelty-based, prediction-based, and learning-progress-driven intrinsic rewards (Oudeyer and Kaplan, 2007; Baranes and Oudeyer, 2009), preserving their advantages while reducing overestimation of purely stochastic experiences. The resulting approach has the potential to support real-world applications in robotics and other complex decision-making tasks, where partial randomness and sparse extrinsic rewards frequently hinder learning efficiency (Todorov et al., 2012).
