### 2.4 Procedural Generation and Robust Exploration in Stochastic Environments

Modern reinforcement learning (RL) research has increasingly focused on procedurally generated environments, where core layouts, object placements, or terrain details are randomized from episode to episode (Raileanu & Rocktäschel, 2020). Such tasks present two major challenges. First, they impose a continual generalization requirement: an agent must adapt to new environment instances or “levels” on the fly rather than memorizing a single layout (Raileanu & Rocktäschel, 2020). Second, stochasticity may arise not only from procedural variations but also from partial randomness embedded in dynamics—causing transitions that defy deterministic modeling (Mavor-Parker et al., 2021). These characteristics complicate the usual count-based or purely forward-model-based exploration, which can be “fooled” by irreducibly random events or ephemeral states (Bellemare et al., 2016; Ostrovski et al., 2017).
