### 2.1 Early Foundations of Intrinsic Motivation

Intrinsic motivation in robotics and machine learning has been discussed extensively, drawing upon psychological notions wherein organisms engage in behaviors "for their inherent satisfaction" rather than explicit external reward signals (Oudeyer & Kaplan, 2007). Early works examined how agents could self-generate rewards from novelty, prediction error, or learning progress to drive exploration in large or sparse-reward environments (Baranes & Oudeyer, 2009; Bellemare et al., 2016). The "noisy TV problem" was recognized early as a significant challenge, wherein purely random or uncontrollable state transitions can inflate naive novelty-based signals (Burda et al., 2018).

A fundamental typology of intrinsic motivation delineates knowledge-based approaches (novelty, surprise, information gain about the environment), competence-based approaches (measuring progress in achieving self-generated goals), and morphological approaches (direct properties of the agent's own sensorimotor flow) (Oudeyer & Kaplan, 2007). Within knowledge-based categories, some methods track state-space densities (Bellemare et al., 2016; Ostrovski et al., 2017), while others emphasize forward-model errors (Pathak et al., 2017). Competence-based systems concentrate on self-set goals and measure improvements in accomplishing those goals, captured by "learning progress" signals (Baranes & Oudeyer, 2009). This theoretical foundation underscores how an agent might differentiate truly learnable or controllable aspects of the environment from purely random ones (Mavor-Parker et al., 2021).
