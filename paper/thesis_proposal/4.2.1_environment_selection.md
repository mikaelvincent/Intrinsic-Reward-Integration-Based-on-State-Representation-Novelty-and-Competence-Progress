#### 4.2.1 Environment Selection

A broad set of environments was selected to evaluate the proposed curiosity-based approaches across discrete, continuous, low-dimensional, and high-dimensional tasks. Each environment poses distinct exploration challengesâ€”often compounded by sparse rewards or large state spaces.

The main environments are outlined below:
1. Mountain Car
   - A classical benchmark frequently used in early reinforcement learning research. It has a two-dimensional continuous state space \(\bigl(position, velocity\bigr)\) and a discrete action space (three actions: move left, move right, or no-op). The task is to drive an underpowered car up a hill, receiving a sparse reward only upon reaching the goal at the top. This setup is known to require deliberate exploration strategies rather than naive random actions.
2. Bipedal Walker
   - A 2D continuous-control environment involving a four-jointed robot leg system. Observations include joint angles, joint angular velocities, hull angle, ground contacts, and sensor readings. Actions are continuous torques at each joint. The reward is generally sparse or delayed, especially if custom or partial reward shaping is disabled. This environment is considered a mid-level complexity benchmark to test motor-control exploration.
3. Car Racing
   - A top-down racing environment with either continuous or discrete actions for steering, acceleration, and braking. State observations are 2D images (RGB) of the track from a top-down perspective. Reward is typically sparse or partial, granted upon visiting new road tiles, which creates exploration challenges in large, visually rich state spaces.
4. Ant (MuJoCo)
   - A four-legged agent (eight degrees of freedom) simulated in a physics engine. The agent's goal is to learn stable locomotion strategies in a continuous state-action space. Observations contain robot joint angles, velocities, and optional contact forces. Actions are continuous torques at each joint. The environment is handled by MuJoCo, an efficient physics simulator (Todorov et al., 2012).
5. Half Cheetah (MuJoCo)
   - A planar "half-cheetah" robot with six degrees of freedom. Like Ant, it is simulated in MuJoCo with continuous states and actions. The environment typically rewards forward velocity in a sparse or delayed fashion, requiring the agent to discover stable gaits through exploration.
6. Humanoid (MuJoCo)
   - A high-dimensional continuous-control environment featuring a 3D humanoid robot with 17 or more torque-controlled joints. The agent must maintain balance and learn to walk in a stable manner. This environment is significantly more challenging due to high dimensionality and potential partial observability from fast dynamic motions.

The selected set offers diversity in state dimensionality, action representation, and reward structures. Mountain Car, Bipedal Walker, and Car Racing exemplify smaller-scale or 2D tasks, while Ant, Half Cheetah, and Humanoid push exploration to larger, more complex continuous-control domains.
