##### 4.4.1.1 PPO Hyperparameters

- Learning Rate (\(\alpha\))
  - Range: \(3\times10^{-4}\) to \(1\times10^{-3}\).
  - Rationale: Lower values (e.g., \(3\times10^{-4}\)) often yield stable convergence, while higher values (e.g., \(1\times10^{-3}\)) can speed up learning but risk instability.
- PPO Clipping Range (\(\epsilon\))
  - Range: 0.1 to 0.2.
  - Rationale: A smaller clipping range can stabilize updates but may slow learning progress; a slightly larger range can accelerate training but increase variance.
- Discount Factor (\(\gamma\))
  - Range: 0.99 to 0.995.
  - Rationale: A slightly higher discount factor encourages longer-horizon credit assignment, which can be beneficial in sparse-reward tasks.
- GAE Parameter (\(\lambda\))
  - Range: 0.90 to 0.95.
  - Rationale: Lower values (0.90) reduce variance of advantage estimates, while higher values (0.95) can better propagate rewards over longer horizons.
- Batch Size
  - Range: 2048 to 4096 transitions per update.
  - Rationale: Larger batches increase stability but also increase wall-clock time per policy update.
- Mini-Batch Size
  - Range: 64 to 256.
  - Rationale: The mini-batch size used during the policy and value network update can be tuned for balance between stable gradient estimates and computational efficiency.
- Entropy Coefficient
  - Range: 0.0 to 0.01.
  - Rationale: May encourage diversity in the policy; set to a smaller value if intrinsic rewards already drive exploration strongly.
- Number of Epochs per Update
  - Range: 3 to 10.
  - Rationale: Repeated minibatch passes can improve data usage but risk policy overfitting to recent rollouts.
