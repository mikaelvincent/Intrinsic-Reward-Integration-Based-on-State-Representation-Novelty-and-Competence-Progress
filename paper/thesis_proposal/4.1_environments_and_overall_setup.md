### 4.1 Environments and Overall Setup

We conduct experiments in five distinct tasks spanning discrete puzzles, high-dimensional continuous control, and a notoriously sparse Atari game. Each environment is partially randomized or employs procedurally varying states, reflecting the practical challenge that naive exploration often fails in stochastic or large state spaces (Ladosz et al., 2022). Below are the tasks, training protocols, and evaluation details. All runs use three different random seeds \(\{0, 1, 2\}\); each seed trains up to a fixed budget of interaction steps or frames, and no early-stopping heuristic is applied. We periodically log performance metrics, then report the mean and standard deviation over seeds.  

- Door Key (MiniGrid)
  - We adopt the MiniGrid framework (Chevalier-Boisvert et al., 2023). In this 2D grid environment, a locked door must be unlocked using a key. Observations are partially observable, typically a \(7\times7\) window around the agent. An episode ends either on success (door unlocked) or after 200 steps. We train for 2 million environment steps, re-randomizing layouts each episode.  
- Key Corridor (MiniGrid)
  - Also from the MiniGrid suite (Chevalier-Boisvert et al., 2023), the agent navigates corridors to retrieve objects behind locked doors. Partial observability and random seeds produce distinct room placements each episode. We use the "KeyCorridorS3R3" variant with a maximum of 200 steps per episode. The agent is trained for 2 million environment steps, measuring final success rates and intermediate exploration statistics.  
- Montezuma's Revenge (Atari)
  - We include this classic sparse-reward Atari benchmark, often cited for demanding exploration. Each episode runs up to 4500 frames and uses sticky actions with probability 0.25. We record extrinsic game score and track visitation of distinct rooms or items. Training spans 10 million frames.  
- Half Cheetah (MuJoCo)
  - Using the MuJoCo physics engine (Todorov et al., 2012), the agent controls a half-cheetah robot with continuous actions. The default extrinsic reward is proportional to forward velocity, but we also augment it with our intrinsic signals. We run episodes up to 1000 steps each. Total training is 2 million environment steps, with random initial states each episode to ensure varied conditions.  
- Ant Maze (MuJoCo)
  - Here, a four-legged ant robot navigates a 2D maze environment (Todorov et al., 2012). We fix a maze of size 4 with randomly chosen initial positions, and the agent receives extrinsic reward only upon reaching a designated goal region (otherwise 0). Episodes terminate at 500 steps. We train for 3 million steps overall, evaluating success rates of finding the goal.  

In all environments, we repeat each run three times with different random seeds, tracking learning curves of extrinsic performance and (where relevant) coverage metrics such as distinct rooms visited. We do not halt runs prematurely; each continues to the predefined budget. By including both discrete MiniGrid tasks and continuous MuJoCo tasks, plus the challenging Montezuma's Revenge domain, we ensure coverage of distinct state-action spaces and varying degrees of partial observability. This setup allows us to evaluate how effectively our adaptive curiosity-driven approach handles partially random or sparse-reward conditions that often hinder naive exploration (Ladosz et al., 2022).  
