#### 3.1.1 Value Functions and the Bellman Equations

A central concept in RL is the **value function**, which captures the agent’s expected return starting from a certain state \(s\) (Szepesvári, 2010). For a policy \(\pi\), the **state-value function** \(V^\pi\) is:

\[
V^\pi(s) \;=\; \mathbb{E}_{\tau\sim\pi}\Bigl[\sum_{t=0}^{\infty}\gamma^t\,r(s_t,a_t)\;\Big|\;s_0=s\Bigr].
\]

Likewise, the **action-value function** \(Q^\pi\) is:

\[
Q^\pi(s,a) \;=\; \mathbb{E}_{\tau\sim\pi}\Bigl[\sum_{t=0}^{\infty}\gamma^t\,r(s_t,a_t)\;\Big|\;s_0=s,\;a_0=a\Bigr].
\]

Under suitable conditions (e.g., bounded rewards, \(\gamma<1\)), these functions satisfy the **Bellman equations** (Szepesvári, 2010):

\[
\begin{aligned}
V^\pi(s) &= \sum_{a\in\mathcal{A}}\;\pi(a\mid s)\;\Bigl[r(s,a) \;+\;\gamma\,\mathbb{E}_{s'\sim P(\cdot\mid s,a)}\bigl[V^\pi(s')\bigr]\Bigr],\\
Q^\pi(s,a) &= r(s,a) \;+\;\gamma\,\mathbb{E}_{s'\sim P(\cdot\mid s,a)}\Bigl[\sum_{a'}\;\pi(a'\mid s')\,Q^\pi(s',a')\Bigr].
\end{aligned}
\]

**Optimal** value functions \(V^*\) and \(Q^*\) are the unique solutions to the **Bellman optimality equations**, e.g.,

\[
Q^*(s,a) \;=\; r(s,a) \;+\;\gamma\,\mathbb{E}_{s'\sim P(\cdot\mid s,a)}\Bigl[\max_{a'\in\mathcal{A}}\,Q^*(s',a')\Bigr].
\]
