#### 2.5.2 Variational Option Discovery and Mutual Information

Beyond modeling environment dynamics, variational option discovery techniques seek to uncover diverse latent skills or “options” (Achiam et al., 2018; Gregor et al., 2016). Under these frameworks, the agent learns a latent variable (skill label) alongside a policy. By maximizing mutual information between the latent skill and visited states (or final trajectories), the agent effectively discovers a spectrum of distinct controllable behaviors in an unsupervised manner. A prime example is Variational Intrinsic Control (VIC) (Gregor et al., 2016), which maximizes the mutual information between skill labels and final states, driving each skill to consistently reach a unique outcome.

These approaches often highlight the importance of closed-loop policies that adapt to partial environment randomness (Jarrett et al., 2022). If the agent is free to adjust its actions based on current states—rather than following a blind open-loop sequence—it remains robust to stochastic transitions and can reliably map each latent skill to a unique, learnable trajectory. In more advanced settings, a trajectory-focused approach such as VALOR (Achiam et al., 2018) employs a recurrent decoder that attempts to reconstruct the agent’s latent skill from entire trajectories, leading to richly distinct behaviors that can later be composed to solve new tasks.
