#### 2.3.1 Motivation for Combining Multiple Signals

A single intrinsic reward channel often falls prey to failure modes when confronted with complex or stochastic environments. For instance, forward-model prediction error can become indefinitely high for uncontrollably random transitions (the “noisy TV” pitfall), while purely count- or density-based methods may struggle if states are rarely revisited and thus remain perpetually “novel” (Burda, Edwards, Storkey, & Klimov, 2018; Pathak et al., 2019). As environments grow more diverse (e.g., procedurally generated grids, large open worlds), each method alone might yield sparse or noisy intrinsic feedback. Consequently, researchers have increasingly explored **multi-signal** curiosity designs (Huang et al., 2021) that merge, adaptively weight, or temporally schedule separate rewards for:

1. **State Novelty** (count-based or distribution-based): Estimating how often the agent has encountered similar states (Bellemare et al., 2016; Ostrovski et al., 2017).  
2. **Predictive Error** (knowledge-based): Tracking how well a learned forward model (or ensemble) predicts the next state (Pathak et al., 2017; Mavor-Parker et al., 2021).  
3. **Bayesian or Ensemble Disagreement**: Modeling epistemic uncertainty and generating exploration bonuses from model variance (Houthooft et al., 2016; Pathak et al., 2019).  
4. **Competence Progress**: Rewarding improvements in self-generated goal achievement or skill mastery over time (Baranes & Oudeyer, 2009; Eysenbach et al., 2018).  

Such **hybrid approaches** aim to offset the vulnerabilities of any single channel. When state novelty saturates due to partial randomness or ephemeral observations, for instance, an action-conditional predictive signal may continue driving exploration in more learnable regions. Conversely, if predictive error alone is hampered by environment noise, a count-based or ensemble-based novelty measure might encourage seeking fresh transitions that reduce overall epistemic uncertainty (Groth et al., 2021).
