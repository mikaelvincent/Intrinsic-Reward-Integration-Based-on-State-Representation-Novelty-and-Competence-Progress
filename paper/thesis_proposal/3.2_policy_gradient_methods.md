### 3.2 Policy Gradient Methods

For large or continuous action spaces, policy gradient techniques optimize parameterized policies $\pi_\theta$ via gradient ascent on performance estimates (Kakade, 2001). Actor-critic variants maintain a separate value function approximator, while the actor's parameters are updated toward maximizing an advantage-weighted log probability of selected actions (Peters & Schaal, 2008). Natural policy gradient methods precondition updates by the Fisher information matrix, improving stability and convergence (Kakade, 2001; Peters & Schaal, 2008).

In partially random domains, policy gradient updates often benefit from injecting well-chosen exploration signals. For instance, including a curiosity term $\eta \cdot r^{\mathrm{int}}$ can significantly boost data efficiency, as the agent systematically seeks transitions about which the forward model or state embedding remains uncertain (Houthooft et al., 2016; Pathak et al., 2017).
