### 1.4 Scope and Limitation of the Study

The proposed approach targets partially random reinforcement learning tasks featuring sparse or delayed extrinsic rewards. Environments under consideration include both discrete grid-based scenarios and continuous robot control tasks. The scope encompasses:
- Developing and integrating an intrinsic reward framework that unifies state novelty, forward-model prediction error, and learning progress signals.
- Implementing systematic assessments across a variety of RL benchmarks to evaluate exploration performance and robustness.
- Comparing the proposed exploration strategy against established intrinsic motivation methods from the literature.

This work does not extend to the complete deployment of the proposed method on physical robotic hardware, although future adaptations to real-world systems remain a viable option. Similarly, exploring hierarchical RL or multi-task expansions lies outside the immediate focus of this study but represents a promising avenue for subsequent research.
