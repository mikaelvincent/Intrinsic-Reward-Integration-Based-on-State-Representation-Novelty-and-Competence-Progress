#### 2.3.2 Unified Methods and Adaptive Weighting

Unified Curiosity-Driven Learning with Smoothed Intrinsic Reward Estimation (Huang et al., 2021) exemplifies how multiple signals can be merged, carefully assigning dynamic weights to each component. One might combine:
- A state-focused novelty term (e.g., random-network-based state embedding)
- An action-conditional forward-model error term
- A smoothed neighbor-based correction to reduce high variance in the intrinsic reward

By adapting these weights based on the local distribution of visited states or current policy entropy, the agent avoids either ignoring a new region too early or repeatedly chasing environment noise. Similarly, some frameworks use competence-based progress in tandem with knowledge-based noveltyâ€”providing an additional boost whenever the agent demonstrates learning progress in mastering sub-goals (Baranes & Oudeyer, 2009; Han et al., 2019). These weighted or fused reward signals may substantially stabilize learning in large or partially random tasks.
