#### 2.5.1 Bayesian Information Gain and Self-Supervised Dynamics

One prominent strategy is to view agent–environment interactions as an opportunity to reduce uncertainty in a learned dynamics model. Variational Information Maximizing Exploration (VIME) (Houthooft et al., 2016) exemplifies this perspective by maintaining a Bayesian neural network that captures the agent’s belief over environment dynamics. After each transition, the agent updates the network’s posterior; the intrinsic reward is the KL divergence between the old and new parameter distributions. This bonus reflects information gain—how much that transition refines the agent’s belief. As the agent explores, truly unlearnable or purely random transitions quickly provide diminishing returns, mitigating so-called “noisy TV” traps.

Similar ideas manifest in ensemble-based self-supervised approaches. For instance, measuring model disagreement (Pathak et al., 2019) can drive exploration: an agent trains multiple forward models and computes intrinsic rewards from the variance in their predictions. As environment randomness becomes recognized, these models converge to the same uncertain prediction, so the agent no longer receives high novelty bonuses for uncontrollable outcomes. This ensemble disagreement has been applied effectively to continuous-control tasks and partially random domains, showcasing how purely internal motivations can lead to systematic coverage of learnable transitions.
