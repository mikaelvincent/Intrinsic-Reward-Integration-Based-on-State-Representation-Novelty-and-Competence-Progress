#### 3.4.3 Skill Discovery via Mutual Information

Beyond sampling discrete “goals,” competence-based exploration also intersects with skill discovery, wherein an agent autonomously learns a family of latent-conditioned policies capable of reliably achieving distinct outcomes (Eysenbach et al., 2018; Gregor et al., 2016; Achiam et al., 2018).

For instance, Variational Intrinsic Control (VIC) (Gregor et al., 2016) maximizes the mutual information between a skill index \(z\) and the final state \(s_{\mathrm{final}}\):
\[
I(z; s_{\mathrm{final}})
\;=\;
\underbrace{H(z)}_{\text{prior}} 
\;-\;
\underbrace{H\bigl(z\mid s_{\mathrm{final}}\bigr)}_{\text{decodability}}.
\]
By encouraging the decoder to infer which skill was executed from the final state alone, each skill \(\pi_\theta(\cdot\mid z)\) becomes competent at producing a uniquely identifiable outcome—mirroring competence-based approaches (Oudeyer & Kaplan, 2007).

DIAYN (Diversity is All You Need) (Eysenbach et al., 2018) extends this concept to ensure the entire state visitation distribution under each skill is distinguishable. Meanwhile, VALOR (Variational Option Discovery) (Achiam et al., 2018) uses a recurrent decoder to classify which skill produced the entire trajectory, thereby discovering more complex dynamic modes. In each case, the agent’s intrinsic objective fosters competence in consistently reaching or maintaining each skill’s characteristic states, thus discounting unlearnable randomness.
