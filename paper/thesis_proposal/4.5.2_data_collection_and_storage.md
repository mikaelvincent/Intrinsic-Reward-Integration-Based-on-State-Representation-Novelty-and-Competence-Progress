#### 4.5.2 Data Collection and Storage

The experiments vary in whether on-policy or off-policy methods are used, but general practices remain consistent with prior intrinsic-motivation setups (Baranes and Oudeyer, 2009; Bellemare et al., 2016).

The typical data collection pipeline is:
1. Rollout Generation
   - On-Policy (e.g., PPO-based): The agent interacts with multiple parallel environment instances for a certain number of steps (e.g., 2048 per rollout). These transitions are directly used to update the policy and the intrinsic-reward modules before the next rollout begins.
   - Off-Policy (if used in ablation or baseline): The agent appends transitions to a replay buffer. Batches are sampled from this buffer to update the policy and the forward models or curiosity modules.
2. Storage Format
   - Each transition is stored in tuples: $\{s_t, a_t, r^{\mathrm{ext}}_t, r^{\mathrm{int}}_t, s_{t+1}, \dots\}$.
   - The environment's partial randomness or domain randomization seeds are also recorded at the start of each episode to ensure reproducibility (Raileanu and Rocktäschel, 2020).
3. Parallel vs. Single-Process
   - Most configurations use parallel environments to accelerate data collection, typically launching 4–64 environment instances (depending on environment complexity). This is consistent with recommended practice when combining intrinsic motivation and RL (Burda et al., 2018).
   - For computationally heavier environments (e.g., Humanoid) or local-lab settings, fewer parallel instances may be used, but the procedure remains similar.
4. Buffer Size
   - For off-policy segments or ICM-like modules (Pathak et al., 2017), a replay buffer can be of size up to 1 million transitions for large-scale tasks. Regular uniform sampling is done to train forward or inverse dynamics models.

Data logs (episode returns, policy losses, model errors) are periodically written to disk. The final stored models, logs, and replay data facilitate subsequent evaluation or reproducibility checks (Houthooft et al., 2016).
