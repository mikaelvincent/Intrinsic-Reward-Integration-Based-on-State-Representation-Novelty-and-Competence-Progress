#### 4.5.3 Learning Curves and Statistical Significance

Learning curves are generated to track agent performance over training steps or episodes. The purpose is to observe improvement rates and determine how quickly and stably each method converges. In order to reduce short-term fluctuations, a consistent smoothing technique (e.g., rolling average) is applied to the episodic returns. A predetermined window size for smoothing is used, and the same size is maintained across all methods for consistency.

Plots of these curves reflect the performance trend of each algorithm. In cases of high variance, shaded bands corresponding to the standard deviation or standard error may be included to represent variability across runs. To examine reliability, multiple independent training runs with varied random seeds are conducted for every method.

Averaging results across seeds is intended to capture the mean behavior. Confidence intervals or standard deviations are calculated at either fixed training checkpoints or the final training step. In order to assess whether differences between methods are non-random, each approach's final or intermediate scores are tested with statistical procedures (e.g., a rank-sum test). Results are typically deemed significant if the associated p-values meet a predefined threshold. When performance variations are large, bootstrapping each run's distributions can be applied as an additional test of robustness.
