#### 4.05.3 Algorithmic Complexity Considerations

Our approach involves distinct modules that may introduce additional computational overhead. We discuss key considerations, such as:
- Ensemble Size
  - A large ensemble can better approximate posterior uncertainty for EBIG but raises computational costs.
  - Similarly, for ANAPE, we might only need a small ensemble with explicit aleatoric heads to estimate \(\hat{\Sigma}_{t+1}\) reliably (Mavor-Parker et al., 2021).
- Competence Partitioning
  - Regularly re-partitioning the state or goal space can be done with incremental clustering (like R-IAC) or neural embedding + clustering methods.
  - Each re-partition step has overhead \(\mathcal{O}(N \log N)\) for data size \(N\), but is done at fixed intervals to remain tractable.
- Adaptive Weighting
  - Weight updates require storing short-term statistics of each signal.
  - Additional overhead is minimal (\(\mathcal{O}(1)\) per step).

Overall, the method is more computationally demanding than single-signal curiosity, yet the synergy is expected to yield significant performance gains in partially random tasks, justifying the overhead.
