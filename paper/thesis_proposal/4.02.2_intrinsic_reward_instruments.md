#### 4.02.2 Intrinsic Reward Instruments

While the RL policy interacts with the environment, intrinsic signals form our primary "measurement" of exploration value. These signals serve as curiosity instruments, shaping the agent’s data distribution. We adopt a triadic approach that unifies:
- Bayesian Ensemble Disagreement / Information Gain
  - Extends the concepts of Variational Information Maximizing Exploration (VIME) (Houthooft et al., 2016) and Self-Supervised Ensemble Disagreement (Pathak et al., 2019).
  - We maintain an ensemble of forward models \(\{f_i\}\), each approximating the environment’s dynamics. The agent’s Bayesian approximation of model parameters \(\theta\) tracks posterior updates as new transitions arrive.
  - Information gain about dynamics is computed via approximate KL between old and new ensemble posteriors (Houthooft et al., 2016) or the ensemble’s prediction variance (Pathak et al., 2019).
  - This measure becomes an intrinsic reward: if a transition strongly updates the agent’s posterior, it is considered informative.
  - By design, once a transition is recognized as purely random or unlearnable, the ensemble converges to a stable "average" prediction, dropping disagreement to near zero. This instrumentation quantifies “learnability” in the face of partial randomness (Oudeyer & Kaplan, 2007).
- Aleatoric-Uncertainty-Subtracted Prediction Error
  - Adapts the approach of aleatoric mapping agents (Mavor-Parker et al., 2021). Each forward model predicts a mean \(\hat{\mu}_{t+1}\) and a covariance \(\hat{\Sigma}_{t+1}\).
  - The raw error \(\|\hat{\mu}_{t+1} - s_{t+1}\|^2\) is offset by a penalty \(\eta\cdot\mathrm{Tr}(\hat{\Sigma}_{t+1})\). The curiosity instrument thus measures epistemic error, ignoring irreducibly random or uncontrollable aspects.
  - This instrumentation specifically addresses the "noisy-TV" effect (Bellemare et al., 2016; Burda, Edwards, Pathak, et al., 2018) by devaluing states that exhibit high predicted noise.
  - We log each forward-model update and the derived residual curiosity reward, analyzing how partial randomness is discounted over time.
- Competence-Based Skill Progress
  - Building on R-IAC (Baranes & Oudeyer, 2009) and skill-discovery frameworks like DIAYN (Eysenbach et al., 2018) or VIC (Gregor et al., 2016), we incorporate a competence measure.
  - The agent self-selects subgoals or latent skill codes, tries to achieve them, and measures competence progress (\(\Delta \ell\)). If mastery in a subregion grows, the intrinsic reward for that subregion declines, shifting exploration to subregions with active learning progress (Oudeyer & Kaplan, 2007; Han et al., 2019).
  - Data wise, we record goal attempts, success rates, and the regional progression of competence to see which environment parts remain partially random or unlearnable.

We unify the three signals—call them \(r_{\mathrm{IG}}, r_{\mathrm{Aleatoric}}, r_{\mathrm{Competence}}\)—into a single scalar:
\[
r^i_t = \alpha_t \,r_{\mathrm{IG}} \;+\; \beta_t\,r_{\mathrm{Aleatoric}}
        \;+\; \gamma_t\,r_{\mathrm{Competence}},
\]
where \(\{\alpha_t,\beta_t,\gamma_t\}\) are adaptive weights updated based on policy entropy or local state distributions (Huang et al., 2021; Raileanu & Rocktäschel, 2020). This instrumentation logs the separate partial signals and their final aggregated reward.
