#### 2.4.3 Impact-Driven and Hybrid Methods in Procedural Worlds

Procedurally generated tasks often punish naive coverage-based exploration, especially if large fractions of states are uninteresting or unlearnable. Hence, recent research recommends **impact-driven** or **hybrid** signals:

- **Rewarding Agent-Controllable Changes**: RIDE (Raileanu & Rocktäschel, 2020) computes an intrinsic reward from how much the agent’s actions **change** a latent representation of the environment, discounting repeated visits within an episode. This approach remains effective under procedural generation: an action that opens a door or retrieves a key consistently yields a representation shift, regardless of random layout differences.  
- **Unified Curiosity**: Combining state-novelty and forward-model errors (Huang et al., 2021) or fusing multiple signals (Burda, Edwards, Pathak, et al., 2018; Han et al., 2019) can further stabilize exploration. In partially random grids, smoothing or weighting these signals adaptively helps the agent learn crucial sub-tasks—like unlocking doors or pushing objects—without lingering on purely noisy states (Huang et al., 2021).
