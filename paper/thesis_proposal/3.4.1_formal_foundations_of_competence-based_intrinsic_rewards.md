#### 3.4.1 Formal Foundations of Competence-Based Intrinsic Rewards

Let \(\mathcal{G}\) denote the (potentially vast) set of goals or sub-tasks the agent can self-generate or discover. At each episode \(k\), the agent samples a goal \(g_k \in \mathcal{G}\) (possibly from a learned distribution). Upon completing its attempt, the agent measures a competence score:
\[
\ell_a(g_k) \;\in\; [0,1],
\]
which reflects how close it came to achieving \(g_k\). The simplest choice might be \(\ell_a(g_k) = 1 - d\bigl(s_{\mathrm{final}},\,g_k\bigr)\) for some normalized distance function \(d\).

A core design in competence-based exploration is to define the intrinsic reward for the agent’s policy \(\pi_\theta\) based on competence progress (Baranes & Oudeyer, 2009). Symbolically, if the agent’s average error or “distance to goal” in region \(\mathcal{R}\subset \mathcal{G}\) decreases from \(\mathrm{error}^{\text{old}}_{\mathcal{R}}\) to \(\mathrm{error}^{\text{new}}_{\mathcal{R}}\), then:
\[
r^i_k \;=\;\Delta \bigl(\mathrm{error}_{\mathcal{R}}\bigr)
\;=\;\mathrm{error}^{\text{old}}_{\mathcal{R}}\;-\;\mathrm{error}^{\text{new}}_{\mathcal{R}},
\]
where a positive difference indicates learning progress (Oudeyer & Kaplan, 2007). The agent thus focuses on subregions \(\mathcal{R}\) yielding the greatest improvement in task mastery, ignoring parts of the environment that are either already trivial or hopelessly random (Baranes & Oudeyer, 2009).
