##### 4.1.2.4 Final Intrinsic Reward Calculation

Every transition's intrinsic reward $r_{\mathrm{int}}$ combines:

1. Learning Progress $\mathrm{LP}_i$ for the visited subregion $R_i$.
2. Impact-Driven Difference in the learned embedding.

A general form can be:
$$
r_{\mathrm{int}}(s_t,a_t,s_{t+1})=
\alpha_{\mathrm{LP}} \,\mathrm{LP}_i
\;+\;
\alpha_{\mathrm{impact}}
\;\frac{\|\phi(s_{t+1}) - \phi(s_t)\|}{1 + N_{\mathrm{ep}}(s_{t+1})},
$$
where $N_{\mathrm{ep}}(\cdot)$ is the episodic visitation count for state embeddings. Subregion $R_i$ yields zero or negligible bonus if flagged as unlearnable. Coefficients $\alpha_{\mathrm{LP}}$ and $\alpha_{\mathrm{impact}}$ balance these terms according to environment scale and noise levels.
